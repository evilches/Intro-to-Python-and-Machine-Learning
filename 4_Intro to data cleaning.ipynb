{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 1: Introduction to Python"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Python basics\n\nThis section will provide you with a foundational understanding of Python syntax and data types."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Comments\n\nMany of the examples in this notebook include comments. Comments in Python start with the hash character (`#`) and extend to the end of the physical line. A comment may appear at the start of a line or following white space or code, but not within a string literal. A hash character within a string literal is just a hash character. Because comments are there to clarify code and are not interpreted by Python, they may be omitted when typing in examples. For example:"
    },
    {
      "metadata": {
        "trusted": false,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# this is the first comment\nspam = 1  # and this is the second comment\n          # ... and now a third!\ntext = \"# This is not a comment because it's inside quotes.\"\nprint(text)",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "# This is not a comment because it's inside quotes.\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Arithmetic and numeric types\n\n> **Learning goal:** By the end of this subsection, you should be comfortable with using numeric types in Python arithmetic.\n\nPython is an interpreted language, which means that you can interactively use the interpreter to get immediate results. You can see this by using the Python interpreter as a simple calculator: type an expression, and you can see the output immediately.\n\nHow can you see the results? The Python interpreter runs inside this notebook. To run the code inside a cell, either click the **Run Cell** button at the top of the window or press **Ctrl**+**Enter**. Try running the contents of the cell below. (Don't worry, we'll cover what the syntax of the Python code means later on in this section.)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Hello, world.\")",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Hello, world.\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Python numeric operators\n\nExpression syntax is straightforward: the operators `+`, `-`, `*`, and `/` work just like in most other programming languages (such as Java or C). For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "2 + 3",
      "execution_count": 3,
      "outputs": [
        {
          "data": {
            "text/plain": "5"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The order of operations also works as in other programming languages (and in math class):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "30 - 4*5",
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "text/plain": "10"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note what happens when you use division:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "7 / 5",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "1.4"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Division (`/`) always returns a floating-point number, which brings up a good point. Python (like other programming languages) has different numeric types. Integer numbers (such as `1`, `3`, and `20`) have type [`int`](https://docs.python.org/3.6/library/functions.html#int). Numbers with a fractional component (such as `3.0` or `1.6`) have type [`float`](https://docs.python.org/3.5/library/functions.html#float).\n\nYou can mix numeric types in calculations:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "3 * 3.5",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "10.5"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "7.0 / 5",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "1.4"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can perform a type of division that returns an integer: [floor division](https://docs.python.org/3.6/glossary.html#term-floor-division). Floor division uses the `//` operator, discards any remainders, and just returns an `int`."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "7 // 5",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "1"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To calculate the remainder, you can use the modulo operator, `%`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "7 % 5",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "2"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For exponents, use the `**` operator. For example, you can write $5^2$ as:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "5 ** 2",
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/plain": "25"
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Conversely, $2^5$ would be:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "2 ** 5",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "32"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that `**` has higher precedence in the order of operations than the negative sign, `-`. This means that $-5^2$ is actually the same thing as $-\\left(5^2\\right)$:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "-5**2",
      "execution_count": 12,
      "outputs": [
        {
          "data": {
            "text/plain": "-25"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In order to assert the order of precedence that you want, use parentheses, `()`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "(-5)**2",
      "execution_count": 13,
      "outputs": [
        {
          "data": {
            "text/plain": "25"
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Parentheses can supersede the order of operations in any calculation you need to run:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "(30 - 4)*5",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "130"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Variables\n\nAs in other programming languages, it is often essential to save values for later using variables in Python. Python assigns values to variables using the equals sign (`=`):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "length = 15\nwidth = 3 * 5\nlength * width",
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "text/plain": "225"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you come from a programming background in another programming language (such as Java), you might have noticed that we never specified the variable type when we declared our variables `length` and `width`. Python does not require this, and you can change variable types as you wish:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "length = 15\nlength",
      "execution_count": 16,
      "outputs": [
        {
          "data": {
            "text/plain": "15"
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "length = 15.0\nlength",
      "execution_count": 17,
      "outputs": [
        {
          "data": {
            "text/plain": "15.0"
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "length = 'fifteen'\nlength",
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": "'fifteen'"
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that, for all the flexibility of variables in Python, you do have to define them. If you try to use an undefined variable, it will produce an error:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "n",
      "execution_count": 19,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'n' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ab0680a89434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In Python's interactive mode and in Jupyter Notebooks, you can use the built-in variable `_`, which automatically takes the value of the last printed expression. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tax = 11.3 / 100\nprice = 19.95\nprice * tax",
      "execution_count": 20,
      "outputs": [
        {
          "data": {
            "text/plain": "2.25435"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "price + _",
      "execution_count": 21,
      "outputs": [
        {
          "data": {
            "text/plain": "22.204349999999998"
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that you should always treat the `_` variable as read-only. Explicitly assigning a value to it will create an independent local variable with the same name and will mask the built-in variable (and its behavior).\n\nOur previous output was kind of a mess, however; we generally use only two or fewer decimal points when working with prices. In order to clean this up, we can use a built-in function, `round()`."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "round(_, 2)",
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/plain": "22.2"
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We will cover some of the other functions built into Python and user-defined functions later in this section.\n\nYou do not have to define variables one at a time. You can define multiple variables on a single line, like so:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a, b, c, = 3.2, 1, 6\na, b, c",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "(3.2, 1, 6)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also augment variable assignments. This will be particularly useful when we tackle loops later in this section."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = 5\nx = x + 1  # Un-pythonic variable augmentation\nx += 1  # Pythonic variable augmentation\nx",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Pythonic means code that doesn't just get the syntax right but that follows the conventions of the Python community and uses the language in the way it is intended to be used.\n\nNote that augmented assignment doesn’t have to be by 1 or even just addition. Beyond +=, augmented assignment statements in Python include -=, \\*=, /=, %=, and \\**=. Try playing around with different augmentation assignments until this concept makes sense.\n\nPython supports other types of numbers beyond `int` and `float`, such as [`Decimal`](https://docs.python.org/3.6/library/decimal.html#decimal.Decimal) and [`Fraction`](https://docs.python.org/3.6/library/fractions.html#fractions.Fraction). Python also has built-in support for [complex numbers](https://docs.python.org/3.6/library/stdtypes.html#typesnumeric), which are all beyond the scope of this course."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Expressions\n\nAs with other programming languages, expressions are critical for decision making controlling the logical flow of Python programs. The most fundamental way of doing this in Python is with a comparison operator, such as \"`<`\":"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "2 < 5",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Python supplies serveral comparison operators:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<center>**Python Comparison Operators**</center>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "| Operator |      Description      | Sample Input | Sample Output |\n|:--------:|:---------------------:|:------------:|:-------------:|\n| `<`      | Less than             | `2 < 5`      | `True`        |\n| `>`      | Greater than          | `2 > 5`      | `False`       |\n| `<=`     | Less than or equal    | `2 <= 5`     | `True`        |\n|          |                       | `2 <= 2`     | `True`        |\n| `>=`     | Greater than or equal | `2 >= 5`     | `False`       |\n| `==`     | Equality              | `2 == 2`     | `True`        |\n|          |                       | `2 == 5`     | `False`       |\n| `!=`     | Inequality            | `2 != 5`     | `True`        |\n|          |                       | `2 != 2`     | `False`       |"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Python does not restrict you to comparing just two operands at a time. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "a, b, c = 1, 2, 3\na < b < c",
      "execution_count": 1,
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This entire expression is `True` because `1 < 2` is `True` and `2 < 3` is `True`.\n\nYou can also use built-in functions in Python for comparing data. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "min(3, 2.4, 5)",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "2.4"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "max(3, 2.4, 5)",
      "execution_count": 3,
      "outputs": [
        {
          "data": {
            "text/plain": "5"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also combine comparison operators into compound expressions. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "1 < 2 and 2 < 3",
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This compound expression returned `True` because **both** `1 < 2` is true and `2 < 3` is true. (Note that this is equivalent to `1 < 2 < 3`.)\n\n> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now flip around one of the simple expressions and see if the output matches your expectations:\n1 < 2 and 3 < 2",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "False"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Python also provides the `or` Boolean operator, which requires that only one simple expression in a compound expression be true in order to return `True`. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "1 < 2 or 1 > 2",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, `not` inverts the truth evaluation of an expression, such as in:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "not (2 < 3)",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "False"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Play around with compound expressions.\n# Set i to different values to see what results this complex compound expression returns:\ni = 7\n(i == 2) or not (i % 2 != 0 and 1 < i < 5)",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Arithmetic operations on numeric data form the foundation of data science work in Python. Even sophisticated numeric operations are predicated on these basics, so mastering them is essential to doing data science."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Strings\n\n> **Learning goal:** By the end of this subsection, you should be comfortable working with strings at a basic level in Python.\n\nBesides numbers, Python can also manipulate strings. Strings can be enclosed in single quotes (`'...'`) or double quotes (`\"...\"`) with the same result. Use `\\` to escape quotes; that is, use `\\` in order to use quotation marks within the string itself:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "'spam eggs'  # Single quotes.",
      "execution_count": 1,
      "outputs": [
        {
          "data": {
            "text/plain": "'spam eggs'"
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "'doesn\\'t'  # Use \\' to escape the single quote...",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "\"doesn't\""
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\"doesn't\"  # ...or use double quotes instead.",
      "execution_count": 3,
      "outputs": [
        {
          "data": {
            "text/plain": "\"doesn't\""
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the interactive interpreter and Jupyter Notebooks, the output string is enclosed in quotes and special characters are escaped with backslashes. Although this output sometimes looks different from the input (the enclosing quotes could change), the two strings are equivalent. The string is enclosed in double quotes if the string contains a single quote and no double quotes; otherwise, it’s enclosed in single quotes. The [`print()`](https://docs.python.org/3.6/library/functions.html#print) function produces a more readable output by omitting the enclosing quotes and by printing escaped and special characters:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "'\"Isn\\'t,\" she said.'",
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "text/plain": "'\"Isn\\'t,\" she said.'"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('\"Isn\\'t,\" she said.')",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "\"Isn't,\" she said.\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you don't want escaped characters (prefaced by `\\`) to be interpreted as special characters, use *raw strings* by adding an `r` before the first quote:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print('C:\\some\\name')  # Here \\n means newline!",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "C:\\some\name\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(r'C:\\some\\name')  # Note the r before the quote.",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "C:\\some\\name\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### String literals\n\nString literals can span multiple lines and are delineated by triple-quotes: `\"\"\"...\"\"\"` or `'''...'''`.\n\nBecause Python doesn't provide a means for creating multi-line comments, developers often just use triple quotes for this purpose. In a Jupyter notebook, however, such quotes define a string literal that appears as the output of a code cell:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "\"\"\"\nEverything between the first three quotes, including new lines,\nis part of the multi-line comment. Technically, the Python interpreter\nsimply sees the comment as a string, and because it's not otherwise\nused in code, the string is ignored. Convenient, eh?\n\"\"\"",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "\"\\nEverything between the first three quotes, including new lines,\\nis part of the multi-line comment. Technically, the Python interpreter\\nsimply sees the comment as a string, and because it's not otherwise\\nused in code, the string is ignored. Convenient, eh?\\n\""
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For this reason, it's best in notebooks to use the # comment character at the beginning of each line, or better still, just use a Markdown cell outside of a code cell in a Jupyter notebook!\n\nStrings can be *concatenated* (glued together) with the + operator, and repeated with *:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# 3 times 'un', followed by 'ium'\n3 * 'un' + 'ium'",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "'unununium'"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The order of operations applies to operators when they are used with strings as well as numeric types. Try experimenting with different combinations and orders of operators and strings to see what happens."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Concatenating strings\n\nTwo or more *string literals* placed next to each other are automatically concatenated:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "'Py' 'thon'",
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/plain": "'Python'"
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "However, to concatenate variables or a variable and a literal, use `+`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "prefix = 'Py'\nprefix + 'thon'",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "'Python'"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### String indexes\n\nStrings can be *indexed* (subscripted), with the first character having index 0. There is no separate character type; a character is simply a string of size one:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word = 'Python'\nword[0]  # Character in position 0.",
      "execution_count": 12,
      "outputs": [
        {
          "data": {
            "text/plain": "'P'"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[5]  # Character in position 5.",
      "execution_count": 13,
      "outputs": [
        {
          "data": {
            "text/plain": "'n'"
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Indices may also be negative numbers, which means to start counting from the end of the string. Note that because -0 is the same as 0, negative indices start from -1:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[-1]  # Last character.",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "'n'"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[-2]  # Second-last character.",
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "text/plain": "'o'"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[-6]",
      "execution_count": 16,
      "outputs": [
        {
          "data": {
            "text/plain": "'P'"
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Slicing strings\n\nIn addition to indexing, which extracts individual characters, Python also supports *slicing*, which extracts a substring. To slice, you indicate a *range* in the format `start:end`, where the start position is included but the end position is excluded:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[0:2]  # Characters from position 0 (included) to 2 (excluded).",
      "execution_count": 17,
      "outputs": [
        {
          "data": {
            "text/plain": "'Py'"
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[2:5]  # Characters from position 2 (included) to 5 (excluded).",
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": "'tho'"
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you omit either position, the default start position is 0 and the default end is the length of the string:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[:2]   # Character from the beginning to position 2 (excluded).",
      "execution_count": 19,
      "outputs": [
        {
          "data": {
            "text/plain": "'Py'"
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[4:]  # Characters from position 4 (included) to the end.",
      "execution_count": 20,
      "outputs": [
        {
          "data": {
            "text/plain": "'on'"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[-2:] # Characters from the second-last (included) to the end.",
      "execution_count": 21,
      "outputs": [
        {
          "data": {
            "text/plain": "'on'"
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This characteristic means that `s[:i] + s[i:]` is always equal to `s`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[:2] + word[2:]",
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/plain": "'Python'"
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[:4] + word[4:]",
      "execution_count": 23,
      "outputs": [
        {
          "data": {
            "text/plain": "'Python'"
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of *n* characters has index *n*. For example:"
    },
    {
      "metadata": {},
      "cell_type": "raw",
      "source": " +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n 0   1   2   3   4   5   6\n-6  -5  -4  -3  -2  -1"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The first row of numbers gives the position of the indices 0–6 in the string; the second row gives the corresponding negative indices. The slice from *i* to *j* consists of all characters between the edges labeled *i* and *j*, respectively.\n\nFor non-negative indices, the length of a slice is the difference of the indices, if both are within bounds. For example, the length of `word[1:3]` is 2.\n\nAttempting to use an index that is too large results in an error:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[42]  # The word only has 6 characters.",
      "execution_count": 24,
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "string index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e894f93573ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# The word only has 6 characters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: string index out of range"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "However, when used in a range, an index that's too large defaults to the size of the string and does not give an error. This characteristic is useful when you always want to slice at a particular index regardless of the length of a string:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[4:42]",
      "execution_count": 25,
      "outputs": [
        {
          "data": {
            "text/plain": "'on'"
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[42:]",
      "execution_count": 26,
      "outputs": [
        {
          "data": {
            "text/plain": "''"
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Python strings are [immutable](https://docs.python.org/3.6/glossary.html#term-immutable), which means they cannot be changed. Therefore, assigning a value to an indexed position in a string results in an error:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[0] = 'J'",
      "execution_count": 27,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'str' object does not support item assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-91a956888ca7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'J'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The following cell also produces an error:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[2:] = 'py'",
      "execution_count": 28,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'str' object does not support item assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6488bbf78f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A slice is itself a value that you can concatenate with other values using `+`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "'J' + word[1:]",
      "execution_count": 29,
      "outputs": [
        {
          "data": {
            "text/plain": "'Jython'"
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[:2] + 'Py'",
      "execution_count": 30,
      "outputs": [
        {
          "data": {
            "text/plain": "'PyPy'"
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A slice, however, is not a string literal, and it cannot be used with automatic concatenation. The following code produces an error:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "word[:2] 'Py'    # Slice is not a literal; produces an error",
      "execution_count": 31,
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-31-60be1c701626>, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-60be1c701626>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    word[:2] 'Py'    # Slice is not a literal; produces an error\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Oftentimes, while working with strings, it can be useful to evaluate the length of a string. The built-in function [`len()`](https://docs.python.org/3.5/library/functions.html#len) returns the length of a string:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "s = 'supercalifragilisticexpialidocious'\nlen(s)",
      "execution_count": 32,
      "outputs": [
        {
          "data": {
            "text/plain": "34"
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another useful built-in function for working with strings is [`str()`](https://docs.python.org/3.6/library/stdtypes.html#str). This function takes any object and returns a printable string version of that object. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "str(2)",
      "execution_count": 33,
      "outputs": [
        {
          "data": {
            "text/plain": "'2'"
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "str(2.5)",
      "execution_count": 34,
      "outputs": [
        {
          "data": {
            "text/plain": "'2.5'"
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Operations on string data form the other fundamental task you will do in data science in Python. Becoming comfortable with strings now will pay large dividends to you later as you work with increasingly complex data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Other data types\n\n> **Learning goal:** By the end of this subsection, you should have a basic understanding of the remaining fundamental data types in Python and an idea of how and when to use them.\n\nThe string and numeric data types that we have looked at so far are common to many programming languages. The other data types that we will now look at--lists, tuples, and dictionaries--set Python apart from C++ or Java by providing powerful and easy-to-use built-in data structures."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Lists\n\nPython knows a number of compound data types, which are used to group together other values. The most versatile is the [*list*](https://docs.python.org/3.5/library/stdtypes.html#typesseq-list), which can be written as a sequence of comma-separated values (items) between square brackets. Lists might contain items of different types, but usually the items all have the same type."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "squares = [1, 4, 9, 16, 25]\nsquares",
      "execution_count": 35,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 4, 9, 16, 25]"
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Like strings (and all other built-in [sequence](https://docs.python.org/3.5/glossary.html#term-sequence) types), lists can be indexed and sliced:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "squares[0]  # Indexing returns the item.",
      "execution_count": 36,
      "outputs": [
        {
          "data": {
            "text/plain": "1"
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "squares[-1]",
      "execution_count": 37,
      "outputs": [
        {
          "data": {
            "text/plain": "25"
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "squares[-3:]  # Slicing returns a new list.",
      "execution_count": 38,
      "outputs": [
        {
          "data": {
            "text/plain": "[9, 16, 25]"
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "All slice operations return a new list containing the requested elements. This means that the following slice returns a new (shallow) copy of the list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "squares[:]",
      "execution_count": 39,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 4, 9, 16, 25]"
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Lists also support concatenation with the `+` operator:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "squares + [36, 49, 64, 81, 100]",
      "execution_count": 42,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Unlike strings, which are [immutable](https://docs.python.org/3.5/glossary.html#term-immutable), lists are a [mutable](https://docs.python.org/3.5/glossary.html#term-mutable) type, which means you can change any value in the list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "cubes = [1, 8, 27, 65, 125]  # Something's wrong here ...\n4 ** 3  # the cube of 4 is 64, not 65!",
      "execution_count": 43,
      "outputs": [
        {
          "data": {
            "text/plain": "64"
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "cubes[3] = 64  # Replace the wrong value.\ncubes",
      "execution_count": 44,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 8, 27, 64, 125]"
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can assign to slices, which can change the size of the list or clear it entirely:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\nletters",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "['a', 'b', 'c', 'd', 'e', 'f', 'g']"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Replace some values.\nletters[2:5] = ['C', 'D', 'E']\nletters",
      "execution_count": 3,
      "outputs": [
        {
          "data": {
            "text/plain": "['a', 'b', 'C', 'D', 'E', 'f', 'g']"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now remove them.\nletters[2:5] = []\nletters",
      "execution_count": 4,
      "outputs": [
        {
          "data": {
            "text/plain": "['a', 'b', 'f', 'g']"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Clear the list by replacing all the elements with an empty list.\nletters[:] = []\nletters",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "[]"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The built-in [`len()`](https://docs.python.org/3.6/library/functions.html#len) function also applies to lists for getting their lengths:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "letters = ['a', 'b', 'c', 'd']\nlen(letters)",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "4"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can nest lists, which means to create lists that contain other lists. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = ['a', 'b', 'c']\nn = [1, 2, 3]\nx = [a, n]\nx",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "[['a', 'b', 'c'], [1, 2, 3]]"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`x` is a list of lists, and you can access its constituent lists through the same indexing you use with simpler lists:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "x[0]",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "['a', 'b', 'c']"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And by using additional index numbers, you can directly access elements within those sub-lists:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "x[0][0]",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "'a'"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Nested lists come up a lot in programming, so it pays to practice.\n# Which indices would you include after x to get ‘c’?\n# How about to get 3?\n",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### List object methods\n\nPython includes a number of handy functions that are available to all lists.\n\nFor example, [`append()`](https://docs.python.org/3.6/tutorial/datastructures.html) and [`extend()`](https://docs.python.org/3.6/tutorial/datastructures.html) enable you to add to the end of a list, much like the `+=` operator:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles = ['John', 'Paul']\nbeatles.append('George')\nbeatles",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "['John', 'Paul', 'George']"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that you did not actually pass a list to `append()`; passing a list to `append()` results in this behavior:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles2 = ['John', 'Paul', 'George']\nbeatles2.append(['Stuart', 'Pete'])\nbeatles2",
      "execution_count": 12,
      "outputs": [
        {
          "data": {
            "text/plain": "['John', 'Paul', 'George', ['Stuart', 'Pete']]"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To tack a list on the end of an existing list, use `extend()` instead:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.extend(['Stuart', 'Pete'])\nbeatles",
      "execution_count": 13,
      "outputs": [
        {
          "data": {
            "text/plain": "['John', 'Paul', 'George', 'Stuart', 'Pete']"
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "[`index()`](https://docs.python.org/3.6/tutorial/datastructures.html) returns the index of the first matching item in a list (if present):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.index('George')",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "2"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The [`count()`](https://docs.python.org/3.6/tutorial/datastructures.html) method returns the number of items in a list that match objects you pass in:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.count('John')",
      "execution_count": 15,
      "outputs": [
        {
          "data": {
            "text/plain": "1"
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are two methods for removing items from a list. The first is [`remove()`](https://docs.python.org/3.6/tutorial/datastructures.html), which locates the first occurrence of an item in the list and removes it (if present):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.remove('Stuart')\nbeatles",
      "execution_count": 16,
      "outputs": [
        {
          "data": {
            "text/plain": "['John', 'Paul', 'George', 'Pete']"
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The other method for removing items from lists is the [`pop()`](https://docs.python.org/3.6/tutorial/datastructures.html) method. If you supply `pop()` with an index number, it will remove the item from that location in the list and return it; otherwise, `pop()` removes the last item in a list and returns that:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.pop()",
      "execution_count": 17,
      "outputs": [
        {
          "data": {
            "text/plain": "'Pete'"
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The [`insert()`](https://docs.python.org/3.6/tutorial/datastructures.html) method enables you to add an item to a specific location in a list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.insert(1, 'Ringo')\nbeatles",
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": "['John', 'Ringo', 'Paul', 'George']"
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Unsurprisingly, the [`reverse()`](https://docs.python.org/3.6/tutorial/datastructures.html) method reverses the order of items in a list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.reverse()\nbeatles",
      "execution_count": 19,
      "outputs": [
        {
          "data": {
            "text/plain": "['George', 'Paul', 'Ringo', 'John']"
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, the [`sort()`](https://docs.python.org/3.6/tutorial/datastructures.html) method orders the items in a list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "beatles.sort()\nbeatles",
      "execution_count": 20,
      "outputs": [
        {
          "data": {
            "text/plain": "['George', 'John', 'Paul', 'Ringo']"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What happens if you run beatles.extend(beatles)?\n# How about beatles.append(beatles)?\n",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that you can supply your own *lambda function* to `sort()` for use in comparing items in a list. We will cover lambda functions later in this section."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Tuples\n\nAnother immutable data type in Python are *tuples*. It can be useful at times to create a data structure that won't be altered later in a program, such as to protect constant data from being overwritten on accident or to improve performance for iterating over data. This is where tuples come in. You create tuples much as you do lists, only using parentheses instead of brackets."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "t = (1, 2, 3)\nt",
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/plain": "(1, 2, 3)"
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Because tuples are immutable, you cannot change elements within them:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "t[1] = 2.0",
      "execution_count": 23,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'tuple' object does not support item assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-823b86083d61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "However, you can refer to elements within them:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "t[1]",
      "execution_count": 24,
      "outputs": [
        {
          "data": {
            "text/plain": "2"
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also slice tuples:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "t[:2]",
      "execution_count": 25,
      "outputs": [
        {
          "data": {
            "text/plain": "(1, 2)"
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also create tuples from lists:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "l = ['baked', 'beans', 'spam']\nl = tuple(l)\nl",
      "execution_count": 26,
      "outputs": [
        {
          "data": {
            "text/plain": "('baked', 'beans', 'spam')"
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Or you can create lists from tuples:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "l = list(l)\nl",
      "execution_count": 27,
      "outputs": [
        {
          "data": {
            "text/plain": "['baked', 'beans', 'spam']"
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Membership testing\n\nAs your Python programming grows more complex, you will want to test lists and tuples for the membership of specific data. The `in` operator enables you to do that."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tup = ('a', 'b', 'c')\n'b' in tup",
      "execution_count": 1,
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also test to see if something is not in a list or tuple using `not in`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "lis = ['a', 'b', 'c']\n'a' not in lis",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "False"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What happens if you run lis in lis?\n# Is that the behavior you expected?\n# If not, think back to the nested lists we’ve already encountered.\n",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dictionaries\n\nDictionaries in Python provide a means of mapping information between unique keys and values. You create dictionaries by listing zero or more key-value pairs inside of braces, like this:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "capitals = {'France': ('Paris', 2140526)}",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Keys for dictionaries can be three things: strings, numbers, or tuples (that contain only strings, numbers, or other tuples). The important thing is that dictionary keys be immutable, so lists cannot be used for keys in dictionaries, for example.\n\nYou add to dictionaries like this:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "capitals['Nigeria'] = ('Lagos', 6048430)\ncapitals",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "{'France': ('Paris', 2140526), 'Nigeria': ('Lagos', 6048430)}"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now try adding another country (or something else) to the capitals dictionary",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You reference entries much like you do as through an index number for a string, list, or tuple, but instead of an index, use a key:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "capitals['France']",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "('Paris', 2140526)"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "You can also update entries in the dictionary:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "capitals['Nigeria'] = ('Abuja', 1235880)\ncapitals",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "{'France': ('Paris', 2140526), 'Nigeria': ('Abuja', 1235880)}"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "When used on a dictionary, the `len()` method returns the number of keys in a dictionary:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(capitals)",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "2"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Similar to the `pop()` method for lists, the `popitem()` method randomly removes a key from the dictionary, along with its associated value:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "capitals.popitem()",
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/plain": "('Nigeria', ('Abuja', 1235880))"
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "capitals",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "{'France': ('Paris', 2140526)}"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Regardless of how complex and voluminous the data you will work with, these basic data structures will repeatedly be your means for handling and manipulating it. Comfort with these basic data structures is essential to being able to understand and use Python code written by others."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Control flow in Python\n\n> **Learning goal:** By the end of this subsection, you should be comfortable using basic control flows in Python.\n\nNow that you have a working understanding of the fundamental data types and structures in Python, we can move on to actual programming using Python.\n\n#### If-statements\n\n`If` statements in Python are similar to those in other programming languages like Java, and they form the backbone of the logical flow of most programs."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "y = 6\nif y % 2 == 0:\n    print('Even')",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Even\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What behavior do you experience if you change y to be odd?",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did you notice the indentation for print under the if statement? That indentation is important because that is how Python demarks the scope of a control flow--what is contingently run or looped over--as opposed to the braces ({}) used in other languages.\n\nTo cover more contingencies without having to construct a follow-on `if` statement, you can add an `else` statement:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "y = 7\nif y % 2 == 0:\n    print('Even')\nelse:\n    print('Odd')",
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Odd\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`elif` enables you to insert an additional logical test to an `if` statement:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "y = 1\nif y % 2 == 0:\n    print('Even')\nelif y == 1:\n    print('One')\nelse:\n    print('Odd')",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "One\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that, in the previous example, the `if` statement exited after finding the *first* logical test that was `true`. If `y = 1`, and while 1 is indeed odd, the `if` statement executed and exited after finding that `y == 1`, rather than continuing to the end of the statement."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Try changing the value of y in the snippet above.\n# Do you get the output that you expect?\n",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### For-loops\n\nIt is often necessary in programs to iterate over some set of items. This is where `for` loops prove useful. For example, they can provide a useful way to iterate over the items of a list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "colors = ['red', 'yellow', 'blue']\nfor color in colors:\n    print(color)",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "red\nyellow\nblue\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Sometimes, you will want to iterate over a list using the list index rather than items from that list (say, when you want to access items from another list at the same time). In this case, you can combine list-object methods and for loops:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "comp_colors = ['green', 'purple', 'orange']\nfor i in range(len(comp_colors)):\n    print(colors[i], comp_colors[i])",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "red green\nyellow purple\nblue orange\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We've met `len()` before, but [`range()`](https://docs.python.org/3/library/functions.html#func-range) is new to us. That function produces a sequence of integers from 0 to 1 less than the number passed into it. Hence:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for j in range(5):\n    print(j)",
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "0\n1\n2\n3\n4\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In addition to `range(`*`stop`*`)`, the range function can take up to three parameters: `range(`*`start`*, *`stop`*`[, `*step*`])`. This odd-looking notation just means that if you pass a single argument to `range()`, it will take that to be the stop value; two arguments will be the start and stop values; and three values are `start`, `stop`, and `step`.\n\n> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# How would you use range and a for loop to print the sequence of numbers\n# from 10 to 20? How about counting by threes from 17 to 41?\n",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It can also be important to break out of a loop. Python uses the `break` statement borrowed from C to do this. To see this in action, consider two nested for loops:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for n in range(2, 10):\n    for x in range(2, n):\n        if n % x == 0:\n            print(n, 'equals', x, '*', n//x)\n            break\n    else:\n        print(n, 'is a prime number')",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2 is a prime number\n3 is a prime number\n4 equals 2 * 2\n5 is a prime number\n6 equals 2 * 3\n7 is a prime number\n8 equals 2 * 4\n9 equals 3 * 3\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that, in the example above, the `else` statement belongs to the `for` loop, not to the `if` statement.\n\n> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Try changing the code snippet above after you remove the break statement.\n# What output does it now produce?\n",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As part of the control flow of your program, you might want to continue to the next iteration of your loop. The `continue` statement (also borrowed from C) can help with that:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "for num in range(2, 10):\n    if num % 2 == 0:\n        print(\"Found an even number:\", num)\n        continue\n    print(\"Found an odd number:\", num)",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Found an even number: 2\nFound an odd number: 3\nFound an even number: 4\nFound an odd number: 5\nFound an even number: 6\nFound an odd number: 7\nFound an even number: 8\nFound an odd number: 9\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What happens when you replace the continue statement above with a break?\n",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### While-loops\n\nIf we cross the functionality of the `if` statement with that of the `for` loop, we would get the `while` loop, a loop that iterates while some logical condition remains true. Consider this snippet of code to compute the initial sub-sequence of the Fibonacci sequence:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# In the Fibonacci series, the sum of two elements defines the next.\na, b = 0, 1\n\nwhile b < 100:    \n    print(b, end=', ')\n    a, b = b, a+b",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, "
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Go ahead and play with the number of iterations for the while loop. Notice that this snippet also uses multiple assignment for variables.\n\n> **Takeaway:** Control flows are what make programs programs, as opposed to a single sequence of operations. Mastering the logical flow of information in Python will enable you to automate tasks that would be impossibly complex or time-consuming to do manually."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Functions\n\n> **Learning goal:** By the end of this subsection, you should understand how to pass and receive data from functions.\n\nAs in other programming languages, it is often essential in Python to break down your program into reusable chunks. A primary means of doing that is through functions.\n\nFor example, we could rewrite the `while` loop code snippet above as a formal function:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def fib(n):\n    \"\"\"Print a Fibonacci series up to n.\"\"\"\n    a, b = 0, 1\n    while a < n:\n        print(a, end=', ')\n        a, b = b, a+b",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we can call this function and compute the Fibonacci series up to some arbitrary point:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "fib(2000)",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, "
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Python can also define new functions on the fly. These anonymous functions are called *lambda functions* because you define them with the `lambda` keyword. Lambda functions can contain any number of arguments but only one expression."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nums = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nlist(filter(lambda x: x % 2 != 0, nums))",
      "execution_count": 20,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 3, 5, 7, 9]"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** You will constantly be using functions of all kind to perform data science in Python, so understanding how functions accept, work on, and return data is critical to further progress."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### List comprehensions\n\n> **Learning goal:** By the end of this subsection, you should understand how to economically and computationally create lists.\n\nSometimes, it makes more sense to generate a list algorithmically. Consider the last example. We really wanted just a list of numbers from 1 to 10. Rather than type those out, we can use a *list comprehension* to generate it:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "numbers = [x for x in range(1,11)] # Remember to create a range 1 more than the number you actually want.\nnumbers",
      "execution_count": 21,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also perform computation on the items generated for the list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "squares = [x*x for x in range(1,11)]\nsquares",
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can even perform logical tests on list items in the comprehension:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "odd_squares = [x*x for x in range(1,11) if x % 2 != 0]\nodd_squares",
      "execution_count": 23,
      "outputs": [
        {
          "data": {
            "text/plain": "[1, 9, 25, 49, 81]"
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now use a list comprehension to generate a list of odd cubes\n# from 1 to 2,197\n",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** List comprehensions are a popular tool in Python because they enable the rapid, programmatic generation of lists. The economy and ease of use therefore make them an essential tool for you (in addition to a necessary topic to understand as you try to understand Python code written by others)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Classes and instance objects\n\n> **Learning goal:** By the end of this subsection, you should have a basic understanding of class in Python, particularly class methods.\n\nPython is an object-oriented programming language; nearly everything in Python is an object with attributes: data members (variables that belong to that object) and methods (functions built into an object that operate on that object's data).\n\nA [class](https://docs.python.org/3/tutorial/classes.html) is like an object constructor, a blueprint for creating objects.\n\nLet's take a look at what that looks like in Python by creating a class representing a simple bank account."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class BankAccount:\n    \"\"\"Where does your money go?\"\"\"\n    \n    account_count = 0\n    \n    # Above is an example of a class variable\n    # Below are the class methods\n\n    \n    def __init__(self, balance=0):\n        self.balance = balance\n        BankAccount.account_count += 1\n        \n    def deposit(self, amount):\n        self.balance += amount\n        self.display_balance()\n        \n    def withdrawal(self, amount):\n        self.balance -= amount\n        self.display_balance()\n        \n    def display_balance(self):\n        print('New balance: ${:.2f}'.format(self.balance))",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Unless we specify a figure, objects in our `BankAccount` class are created empty. Let's create a new account with \\$50.00 in it:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_account = BankAccount(50)\nmy_account.balance",
      "execution_count": 2,
      "outputs": [
        {
          "data": {
            "text/plain": "50"
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our class also has three methods, two of which are designed to be accessed from outside the object."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_account.deposit(100)",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "New balance: $150.00\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_account.withdrawal(125)",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "New balance: $25.00\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We even have a documentation string in the class:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_account.__doc__",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "'Where does your money go?'"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note the `account_count` class variable. It is outside any method of the class, which means that every instance of this class shares it. In application, that means that every time a new account is created, that counter iterates for every instance of the class. Here's how that looks in action:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "my_account.account_count",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "1"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "your_account = BankAccount()\nprint(my_account.account_count, your_account.account_count)",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2 2\n"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Because nearly everything in Python is an object, it is essential to understand—even at a basic level—what that means and how to use object attributes like methods."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Importing modules\n\n> **Learning goal:** By the end of this subsection, you should be comfortable importing modules in Python.\n\nIf you quit from the Python interpreter and enter it again, the definitions you have made (your functions and variables) will be lost. Similarly, you might also want to use a handy function that you’ve written in several programs without copying its definition into each program.\n\nTo support this, Python has a way to put definitions in a file and use them in a script or in an interactive instance of the interpreter. Such a file is called a [*module*](https://docs.python.org/3/tutorial/modules.html). Definitions from a module can be imported into other programs or modules.\n\nFor example, the `factorial()` function is not one of the standard functions built into Python. It is part of the Python [`math`](https://docs.python.org/3/library/math.html) module. So, when we run `factorial()` before importing `math`, we get an error:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "factorial(5)",
      "execution_count": 8,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'factorial' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-637175d621a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfactorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'factorial' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "However, the situation changes after we import the `math` module:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import math\nmath.factorial(5)",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "120"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that we still have to prepend `math` to the front of the `factorial()` function. We can use a different method to import that specific function from the `math` module and use it as if it were defined in our program:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from math import factorial\nfactorial(5)",
      "execution_count": 10,
      "outputs": [
        {
          "data": {
            "text/plain": "120"
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can add more cells to your notebook by clicking the **insert cell below (+)** button at the top of the window. The Python [`math`](https://docs.python.org/3/library/math.html) module has many functions in it. Try importing some of the other math functions and playing around with them.\n\n> **Takeaway:** There are several Python modules that you will regularly use in conducting data science in Python, so understanding how to import them will be essential (especially in this training)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 2: Introduction to NumPy\n\nNumPy is one of the two most important libraries in Python for data science, along with pandas (which we will cover in the next section). NumPy is a crucial library for effectively loading, storing, and manipulating in-memory data in Python, all of which will be at the heart of what you do with data science in Python.\n\nDatasets come from a huge range of sources and in a wide range of formats, such as text documents, images, sound clips, numerical measurements, and nearly anything else. Despite this variety, however, the start of data science is to think of all data fundamentally as arrays of numbers.\n\nFor example, the words in documents can be represented as the numbers that encode letters in computers or even the frequency of particular words in a collection of documents. Digital images can be thought of as two-dimensional arrays of numbers representing pixel brightness or color. Sound files can be represented as one-dimensional arrays of frequency versus time. However, no matter what form our data takes, in order to analyze it, our first step will be to transform it into arrays of numbers—which is where NumPy comes in (and pandas down the road).\n\nNumPy is short for *Numerical Python*, and it provides an efficient means of storing and operating on dense data buffers in Python. Array-oriented computing in Python goes back to 1995 with the Numeric library. Scientific programming in Python took off over the next 10 years, but the collections of libraries splintered. The NumPy project began in 2005 as a means of bringing the Numeric and NumArray projects together around a single array-based framework.\n\nLet's get started exploring NumPy! Our first step will be to import NumPy using `np` as an alias:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Get used to this convention — it's a common convention in Python, and it's the way we will use and refer to NumPy throughout the rest of this course."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Built-In Help\n\nThere's a lot to learn about NumPy, and it can be tough to remember it all the first time through. Don't worry! IPython — the underlying program that enables notebooks like this one to interact with Python—has you covered.\n\nFirst off, IPython gives you the ability to quickly explore the contents of a package like NumPy by using the tab-completion feature. So, if you want to see all of the functions available with NumPy, type this:\n\n```ipython\nIn [ ]: np.<TAB>\n```\nWhen you do so, a drop-down menu will appear next to the `np.`\n\n> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Place your cursor after the period and press <TAB>:\nnp.",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-f0faa50f3d63>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-f0faa50f3d63>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    np.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "From the drop-down menu, you can select any function to run. Better still, you can select any function and view the built-in help documentation for it. For example, to view the documentation for the NumPy `add()` function, you can run this code:\n\n```ipython\nIn [ ]: np.add?\n```\nTry this with a few different functions. Remember, these functions are just like the ones you wrote in the previous section; the documentation will help explain what parameters you can (or should) provide to the function, in addition to what output you can expect.\n\n> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Replace 'add' below with a few different NumPy function names and look over the documentation:\nnp.add?",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For more detailed documentation (along with additional tutorials and other resources), visit [www.numpy.org](http://www.numpy.org).\n\nNow that you know how to quickly get help while you are working on your own, let's return to storing data in arrays."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## NumPy arrays: a specialized data structure for analysis\n\n> **Learning goal:** By the end of this subsection, you should have a basic understanding of what NumPy arrays are and how they differ from the other Python data structures you have studied thus far.\n\nWe started the discussion in this section by noting that data science starts by representing data as arrays of numbers.\n\n\"Wait!\" you might be thinking. \"Can't we just use Python lists for that?\"\n\nDepending on the data, yes, you could (and you will use lists as a part of working with data in Python). But to see what we might want to use a specialized data structure for, let's look a little more closely at lists."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Lists in Python\n\nPython lists can hold just one kind of object. Let's use one to create a list of just integers:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "myList = list(range(10))\nmyList",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember list comprehension? We can use it to probe the data types of items in a list:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "[type(item) for item in myList]",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "[int, int, int, int, int, int, int, int, int, int]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Of course, a really handy feature of Python lists is that they can hold heterogeneous types of data in a single list object:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "myList2 = [True, \"2\", 3.0, 4]\n[type(item) for item in myList2]",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "[bool, str, float, int]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "However, this flexibility comes at a price. Each item in a list is really a separate Python object (the list is an object itself, true, but mostly it is an object that serves as a container for the memory pointers to the constituent objects). That means that each item in a list must contain its own type info, reference count, and other information. All of this information can become expensive in terms of memory and performance if we are dealing with hundreds of thousands or millions of items in a list. Moreover, for many uses in data science, our arrays just store a single type of data (such as integers or floats), which means that all of the object-related information for items in such an array would be redundant. It can be much more efficient to store data in a fixed-type array.\n\n<img align=\"left\" style=\"padding-right:10px;\" src=\"Graphics/array_vs_list.png\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Fixed-type arrays in Python\n\nAt the level of implementation by the computer, the `ndarray` that is part of the NumPy package contains a single pointer to one contiguous block of data. This is efficient memory-wise and computationally. Better still, NumPy provides efficient *operations* on data stored in `ndarray` objects.\n\n(Note that we will pretty much use “array,” “NumPy array,” and “ndarray” interchangeably throughout this section to refer to the ndarray object.)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Creating NumPy arrays method 1: using Python lists\n\nThere are multiple ways to create arrays in NumPy. Let's start by using our good old familiar Python lists. We will use the `np.array()` function to do this (remember, we imported NumPy as '`np`'):"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an integer array:\nnp.array([1, 4, 2, 5, 3])",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "array([1, 4, 2, 5, 3])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember that, unlike Python lists, NumPy constrains arrays to contain a single type. So, if data types fed into a NumPy array do not match, NumPy will attempt to *upcast* them if possible. To see what we mean, here NumPy upcasts integers to floats:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.array([3.14, 4, 2, 3])",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "array([3.14, 4.  , 2.  , 3.  ])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# What happens if you construct an array using a list that contains a combination of integers, floats, and strings?\n",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you want to explicitly set the data type of your array when you create it, you can use the `dtype` keyword:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "np.array([1, 2, 3, 4], dtype='float32')",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "array([1., 2., 3., 4.], dtype=float32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Try this using a different dtype.\n# Remember that you can always refer to the documentation with the command np.array.\n",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Most usefully for a lot of applications in data science, NumPy arrays can explicitly be multidimensional (like matrices or tensors). Here's one way of creating a multidimensional array using a list of lists:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# nested lists result in multi-dimensional arrays\nnp.array([range(i, i + 3) for i in [2, 4, 6]])",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "array([[2, 3, 4],\n       [4, 5, 6],\n       [6, 7, 8]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The inner lists in a list of lists are treated as rows of the two-dimensional array you created."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Creating NumPy arrays method 2: building from scratch\n\nIn practice, it is often more efficient to create arrays from scratch using functions built into NumPy, particularly for larger arrays. Here are a few examples; these example will help introduce you to several useful NumPy functions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an integer array of length 10 filled with zeros\nnp.zeros(10, dtype=int)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a 3x5 floating-point array filled with ones\nnp.ones((3, 5), dtype=float)",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "array([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a 3x5 array filled with 3.14\n# The first number in the tuple gives the number of rows\n# The second number in the tuple sets the number of columns\nnp.full((3, 5), 3.14)",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "array([[3.14, 3.14, 3.14, 3.14, 3.14],\n       [3.14, 3.14, 3.14, 3.14, 3.14],\n       [3.14, 3.14, 3.14, 3.14, 3.14]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an array filled with a linear sequence\n# Starting at 0, ending at 20, stepping by 2\n# (this is similar to the built-in Python range() function)\nnp.arange(0, 20, 2)",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an array of five values evenly spaced between 0 and 1\nnp.linspace(0, 1, 5)",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a 3x3 array of uniformly distributed\n# random values between 0 and 1\nnp.random.random((3, 3))",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "array([[0.00703713, 0.52877796, 0.75275509],\n       [0.89635637, 0.25126021, 0.44915591],\n       [0.14923241, 0.06215598, 0.80735779]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a 3x3 array of normally distributed random values\n# with mean 0 and standard deviation 1\nnp.random.normal(0, 1, (3, 3))",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "array([[ 0.7714448 ,  0.7676462 , -0.21410694],\n       [ 0.31127101, -1.90044403,  0.88254434],\n       [ 0.76114444,  0.5224406 ,  1.7439717 ]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a 3x3 array of random integers in the interval [0, 10)\nnp.random.randint(0, 10, (3, 3))",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "array([[9, 3, 8],\n       [3, 4, 5],\n       [6, 7, 6]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a 3x3 identity matrix\nnp.eye(3)",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create an uninitialized array of three integers\n# The values will be whatever happens to already exist at that memory location\nnp.empty(3)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "array([0.00000000e+000, 1.93538748e-309, 4.68110620e-310])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now take a couple of minutes to go back and play with these code snippets, changing the parameters. These functions are the bread-and-butter of creating NumPy arrays and you will want to become comfortable with them."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Below is a table listing out several of the array-creation functions in NumPy.\n\n| Function      | Description |\n|:--------------|:------------|\n| `array`       | Converts input data (list, tuple, array, or other sequence type) to an ndarray either |\n|               | by inferring a dtype or explicitly specifying a dtype. Copies the input data by default. |\n| `asarray`     | Converts input to ndarray, but does not copy if the input is already an ndarray. |\n| `arange`      | Similar to the built-in `range()` function but returns an ndarray instead of a list. |\n| `ones`, `ones_like` | Produces an array of all 1s with the given shape and dtype. |\n|               | `ones_like` takes another array and produces a ones-array of the same shape and dtype. |\n| `zeros`, `zeros_like` | Similar to `ones` and `ones_like` but producing arrays of 0s instead. |\n| `empty`, `empty_like` | Creates new arrays by allocating new memory, but does not populate with any values \n|               | like `ones` and `zeros`. |\n| `full`, `full_like` | Produces an array of the given shape and dtype with all values set to the indicated “fill value.” |\n|               | `full_like` takes another array and produces a a filled array of the same shape and dtype. |\n| `eye`, `identity` | Create a square $N \\times N$ identity matrix (1s on the diagonal and 0s elsewhere) |"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### NumPy data types\n\nThe standard NumPy data types are listed in the following table. Note that when constructing an array, they can be specified using a string:\n\n```python\nnp.zeros(8, dtype='int16')\n```\n\nOr they can be specified directly using the NumPy object:\n\n```python\nnp.zeros(8, dtype=np.int16)\n```"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "| Data type\t    | Description |\n|:--------------|:------------|\n| ``bool_``     | Boolean (True or False) stored as a byte |\n| ``int_``      | Default integer type (same as C ``long``; normally either ``int64`` or ``int32``)| \n| ``intc``      | Identical to C ``int`` (normally ``int32`` or ``int64``)| \n| ``intp``      | Integer used for indexing (same as C ``ssize_t``; normally either ``int32`` or ``int64``)| \n| ``int8``      | Byte (-128 to 127)| \n| ``int16``     | Integer (-32768 to 32767)|\n| ``int32``     | Integer (-2147483648 to 2147483647)|\n| ``int64``     | Integer (-9223372036854775808 to 9223372036854775807)| \n| ``uint8``     | Unsigned integer (0 to 255)| \n| ``uint16``    | Unsigned integer (0 to 65535)| \n| ``uint32``    | Unsigned integer (0 to 4294967295)| \n| ``uint64``    | Unsigned integer (0 to 18446744073709551615)| \n| ``float_``    | Shorthand for ``float64``.| \n| ``float16``   | Half-precision float: sign bit, 5 bits exponent, 10 bits mantissa| \n| ``float32``   | Single-precision float: sign bit, 8 bits exponent, 23 bits mantissa| \n| ``float64``   | Double-precision float: sign bit, 11 bits exponent, 52 bits mantissa| \n| ``complex_``  | Shorthand for ``complex128``.| \n| ``complex64`` | Complex number, represented by two 32-bit floats| \n| ``complex128``| Complex number, represented by two 64-bit floats| "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If these data types seem a lot like those in C, that's because NumPy is built in C.\n\n> **Takeaway:** NumPy arrays are a data structure similar to Python lists that provide high performance when storing and working on large amounts of homogeneous data—precisely the kind of data that you will encounter frequently in doing data science. NumPy arrays support many data types beyond those discussed in this course. With all of that said, however, don’t worry about memorizing all of the NumPy dtypes. **It’s often just necessary to care about the general kind of data you’re dealing with: floating point, integer, Boolean, string, or general Python object.**"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Working with NumPy arrays: the basics\n\n> **Learning goal:** By the end of this subsection, you should be comfortable working with NumPy arrays in basic ways.\n\nNow that you know how to create arrays in NumPy, you need to get comfortable manipulating them for two reasons. First, you will work with NumPy arrays as part of your exploration of data science. Second, our other important Python data-science tool, pandas, is actually built around NumPy. Getting good at working with NumPy arrays will pay dividends in the next section and beyond: NumPy arrays are the building blocks for the `Series` and `DataFrame` data structures in the Python pandas library and you will use them *a lot* in data science. To get comfortable with array manipulation, we will cover five specifics:\n- **Arrays attributes**: Assessing the size, shape, and data types of arrays\n- **Indexing arrays**: Getting and setting the value of individual array elements\n- **Slicing arrays**: Getting and setting smaller subarrays within a larger array\n- **Reshaping arrays**: Changing the shape of a given array\n- **Joining and splitting arrays**: Combining multiple arrays into one and splitting one array into multiple arrays"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Array attributes\nFirst, let's look at some array attributes. We'll start by defining three arrays filled with random numbers: one one-dimensional, another two-dimensional, and the last three-dimensional. Because we will be using NumPy's random number generator, we will set a *seed* value in order to ensure that you get the same random arrays each time you run this code:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nnp.random.seed(0)  # seed for reproducibility\n\na1 = np.random.randint(10, size=6)  # One-dimensional array\na2 = np.random.randint(10, size=(3, 4))  # Two-dimensional array\na3 = np.random.randint(10, size=(3, 4, 5))  # Three-dimensional array",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Each array has attributes ``ndim`` (the number of dimensions of an array), ``shape`` (the size of each dimension of an array), and ``size`` (the total number of elements in an array).\n\n> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Change the values in this code snippet to look at the attributes for a1, a2, and a3:\nprint(\"a3 ndim: \", a3.ndim)\nprint(\"a3 shape:\", a3.shape)\nprint(\"a3 size: \", a3.size)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "a3 ndim:  3\na3 shape: (3, 4, 5)\na3 size:  60\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another useful array attribute is the `dtype`, which we already encountered earlier in this section as a means of determining the type of data in an array:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"dtype:\", a3.dtype)",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": "dtype: int64\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Explore the dtype for the other arrays.\n# What dtypes do you predict them to have?\nprint(\"dtype:\", a3.dtype)",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": "dtype: int64\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Indexing arrays\n\nIndexing in NumPy is pretty similar to indexing lists in standard Python. In fact, indices in one-dimensional arrays work exactly as they do with Python lists:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a1",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "array([5, 0, 3, 3, 7, 9])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a1[0]",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "5"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a1[4]",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As with regular Python lists, in order to index from the end of the array, you can use negative indices:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a1[-1]",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "9"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a1[-2]",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Do multidimensional NumPy arrays work like Python lists of lists?\n# Try a few combinations like a2[1][1] or a3[0][2][1] and see what comes back\n",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You might have noticed that we can treat multidimensional arrays like lists of lists. But a more common means of accessing items in multidimensional arrays is to use a comma-separated tuple of indices.\n\n(Yes, we realize that these comma-separated tuples use square brackets rather than the parentheses the name might suggest, but they are nevertheless referred to as tuples.)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 43,
          "data": {
            "text/plain": "array([[3, 5, 2, 4],\n       [7, 6, 8, 8],\n       [1, 6, 7, 7]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2[0, 0]",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 44,
          "data": {
            "text/plain": "3"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2[2, 0]",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 45,
          "data": {
            "text/plain": "1"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2[2, -1]",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 46,
          "data": {
            "text/plain": "7"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also modify values by use of this same comma-separated index notation:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2[0, 0] = 12\na2",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 47,
          "data": {
            "text/plain": "array([[12,  5,  2,  4],\n       [ 7,  6,  8,  8],\n       [ 1,  6,  7,  7]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember, once defined, NumPy arrays have a fixed data type. So, if you attempt to insert a float into an integer array, the value will be silently truncated."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a1[0] = 3.14159\na1",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 48,
          "data": {
            "text/plain": "array([3, 0, 3, 3, 7, 9])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# What happens if you try to insert a string into a1?\n# Hint: try both a string like '3' and one like 'three'\n",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Slicing arrays\nSimilar to how you can use square brackets to access individual array elements, you can also use them to access subarrays. You do this with the *slice* notation, marked by the colon (`:`) character. NumPy slicing syntax follows that of the standard Python list; so, to access a slice of an array `a`, use this notation:\n``` python\na[start:stop:step]\n```\nIf any of these are unspecified, they default to the values ``start=0``, ``stop=``*``size of dimension``*, ``step=1``.\nLet's take a look at accessing sub-arrays in one dimension and in multiple dimensions."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### One-dimensional slices"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = np.arange(10)\na",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 50,
          "data": {
            "text/plain": "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a[:5]  # first five elements",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 51,
          "data": {
            "text/plain": "array([0, 1, 2, 3, 4])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a[5:]  # elements after index 5",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 52,
          "data": {
            "text/plain": "array([5, 6, 7, 8, 9])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a[4:7]  # middle sub-array",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "array([4, 5, 6])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a[::2]  # every other element",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 54,
          "data": {
            "text/plain": "array([0, 2, 4, 6, 8])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a[1::2]  # every other element, starting at index 1",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 55,
          "data": {
            "text/plain": "array([1, 3, 5, 7, 9])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# How would you access the *last* five elements of array a?\n# How about every other element of the last five elements of a?\n# Hint: Think back to list indexing in Python\n",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Be careful when using negative values for ``step``. When ``step`` has a negative value, the defaults for ``start`` and ``stop`` are swapped and you can use this functionality to reverse an array:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a[::-1]  # all elements, reversed",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "text/plain": "array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a[5::-2]  # reversed every other from index 5",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 58,
          "data": {
            "text/plain": "array([5, 3, 1])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# How can you create a slice that contains every third element of a\n# descending from the second-to-last element to the second element of a?\n",
      "execution_count": 59,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Multidimensional slices\n\nMultidimensional slices use the same slice notation of one-dimensional subarrays mixed with the comma-separated notation of multidimensional arrays. Some examples will help illustrate this."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 60,
          "data": {
            "text/plain": "array([[12,  5,  2,  4],\n       [ 7,  6,  8,  8],\n       [ 1,  6,  7,  7]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2[:2, :3]  # two rows, three columns",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 61,
          "data": {
            "text/plain": "array([[12,  5,  2],\n       [ 7,  6,  8]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2[:3, ::2]  # all rows, every other column",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 62,
          "data": {
            "text/plain": "array([[12,  2],\n       [ 7,  8],\n       [ 1,  7]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, subarray dimensions can even be reversed together:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2[::-1, ::-1]",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 63,
          "data": {
            "text/plain": "array([[ 7,  7,  6,  1],\n       [ 8,  8,  6,  7],\n       [ 4,  2,  5, 12]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now try to show 2 rows and 4 columns with every other element?\n",
      "execution_count": 64,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Accessing array rows and columns\nOne thing you will often need to do in manipulating data is accessing a single row or column in an array. You can do this through a combination of indexing and slicing; specifically by using an empty slice marked by a single colon (``:``). Again, some examples will help illustrate this."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(a2[:, 0])  # first column of x2",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[12  7  1]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(a2[0, :])  # first row of x2",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[12  5  2  4]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the case of row access, the empty slice can be omitted for a more compact syntax:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(a2[0])  # equivalent to a2[0, :]",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[12  5  2  4]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# How would you access the third column of a3?\n# How about the third row of a3?\n",
      "execution_count": 68,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Slices are no-copy views\nIt's important to know that slicing produces *views* of array data, not *copies*. This is a **huge** difference between NumPy array slicing and Python list slicing. With Python lists, slices are only shallow copies of lists; if you modify a copy, it doesn't affect the parent list. When you modify a NumPy subarray, you modify the original list. Be careful: this can have ramifications when you are trying to just work with a small part of a large dataset and you don’t want to change the whole thing. Let's look more closely."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(a2)",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[12  5  2  4]\n [ 7  6  8  8]\n [ 1  6  7  7]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Extract a $2 \\times 2$ subarray from `a2`:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2_sub = a2[:2, :2]\nprint(a2_sub)",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[12  5]\n [ 7  6]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now modify this subarray:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2_sub[0, 0] = 99\nprint(a2_sub)",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[99  5]\n [ 7  6]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`a2` is now modified as well:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(a2)",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[99  5  2  4]\n [ 7  6  8  8]\n [ 1  6  7  7]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Now try reversing the column and row order of a2_sub\n# Does a2 look the way you expected it would after that manipulation?\n",
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The fact that slicing produces views rather than copies is useful for data-science work. As you work with large datasets, you will often find that it is easier to access and manipulate pieces of those datasets rather than copying them entirely."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Copying arrays\nInstead of just creating views, sometimes it is necessary to copy the data in one array to another. When you need to do this, use the `copy()` method:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2_sub_copy = a2[:2, :2].copy()\nprint(a2_sub_copy)",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[99  5]\n [ 7  6]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If we now modify this subarray, the original array is not touched:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a2_sub_copy[0, 0] = 42\nprint(a2_sub_copy)",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[42  5]\n [ 7  6]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(a2)",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[99  5  2  4]\n [ 7  6  8  8]\n [ 1  6  7  7]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Reshaping arrays\nAnother way in which you will need to manipulate arrays is by reshaping them. This involves changing the number and size of dimensions of an array. This kind of manipulation can be important in getting your data to meet the expectations of machine learning programs or APIs.\n\nThe most flexible way of doing this kind of manipulation is with the `reshape` method. For example, if you want to put the numbers 1 through 9 in a $3 \\times 3$ grid, you can do the following:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "grid = np.arange(1, 10).reshape((3, 3))\nprint(grid)",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[1 2 3]\n [4 5 6]\n [7 8 9]]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another common manipulation you will do in data science is converting one-dimensional arrays into two-dimensional row or column matrices. This can be a common necessity when doing linear algebra for machine learning. While you can do this by means of the `reshape` method, an easier way is to use the `newaxis` keyword in a slice operation:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = np.array([1, 2, 3])\n\n# row vector via reshape\na.reshape((1, 3))",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 78,
          "data": {
            "text/plain": "array([[1, 2, 3]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# row vector via newaxis\na[np.newaxis, :]",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 79,
          "data": {
            "text/plain": "array([[1, 2, 3]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# column vector via reshape\na.reshape((3, 1))",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 80,
          "data": {
            "text/plain": "array([[1],\n       [2],\n       [3]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# column vector via newaxis\na[:, np.newaxis]",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 81,
          "data": {
            "text/plain": "array([[1],\n       [2],\n       [3]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You will see this type of transformation a lot in the remainder of this course."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Joining and splitting arrays\n\nAnother common data-manipulation need in data science is combining multiple datasets; learning first how to do this with NumPy arrays will help you in the next section when we do this with more complex data structures. You will many times also need to split a single array into multiple arrays."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Joining arrays\nTo join arrays in NumPy, you will most often use `np.concatenate`, which is the method we will cover here. If you find yourself in the future needing to specifically join arrays in mixed dimensions (a rarer case), read the documentation on `np.vstack`, `np.hstack`, and `np.dstack`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### `np.concatenate()`\n\n`np.concatenate` takes a tuple or list of arrays as its first argument:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "a = np.array([1, 2, 3])\nb = np.array([3, 2, 1])\nnp.concatenate([a, b])",
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 82,
          "data": {
            "text/plain": "array([1, 2, 3, 3, 2, 1])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also concatenate more than two arrays at once:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "c = [99, 99, 99]\nprint(np.concatenate([a, b, c]))",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[ 1  2  3  3  2  1 99 99 99]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`np.concatenate` can also be used for two-dimensional arrays:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "grid = np.array([[1, 2, 3],\n                 [4, 5, 6]])",
      "execution_count": 84,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# concatenate along the first axis, which is the default\nnp.concatenate([grid, grid])",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 85,
          "data": {
            "text/plain": "array([[1, 2, 3],\n       [4, 5, 6],\n       [1, 2, 3],\n       [4, 5, 6]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Recall that axes are zero-indexed in NumPy.\n# What do you predict np.concatenate([grid, grid], axis=1) will produce?\n",
      "execution_count": 86,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Splitting arrays\nIn order to split arrays into multiple smaller arrays, you can use the functions ``np.split``, ``np.hsplit``, ``np.vsplit``, and ``np.dsplit``.  As above, we will only cover the most commonly used function (`np.split`) in this course."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "##### `np.split()`\nLet's first examine the case of a one-dimensional array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = [1, 2, 3, 99, 99, 3, 2, 1]\na1, a2, a3 = np.split(a, [3, 5])\nprint(a1, a2, a3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that *N* split-points produces to *N + 1* subarrays. In this case it has formed the subarray `a2` with `a[3]` and `a[4]` (the element just before position 5 [remember how Python indexing goes], the second input in the tuple) as elements. `a1` and `a3` pick up the leftover portions from the original array `a`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "grid = np.arange(16).reshape((4, 4))\ngrid",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What does np.split(grid, [1, 2]) produce?\n# What about np.split(grid, [1, 2], axis=1)?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Manipulating datasets is a fundamental part of preparing data for analysis. The skills you learned and practiced here will form building blocks for the most sophisticated data-manipulation you will learn in later sections in this course."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fancy indexing\n\nSo far, we have explored how to access and modify portions of arrays using simple indices like `arr[0]`) and slices like `arr[:5]`. Now it is time for fancy indexing, in which we pass an array of indices to an array in order to access or modify multiple array elements at the same time.\n\nLet's try it out:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "rand = np.random.RandomState(42)\n\narr = rand.randint(100, size=10)\nprint(arr)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Suppose you need to access three different elements. Using the tools you currently have, your code might look something like this:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "[arr[3], arr[7], arr[2]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With fancy indexing, you can pass a single list or array of indices to do the same thing:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind = [3, 7, 4]\narr[ind]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another useful aspect of fancy indexing is that the shape of the output array reflects the shape of the *index arrays* you supply, rather than the shape of the array you are accessing. This is handy because there will be many times in a data scientist's life when they want to grab data from an array in a particular manner, such as to pass it to a machine learning API. Let's examine this property with an example:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind = np.array([[3, 7],\n                [4, 5]])\narr[ind]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`arr` is a one-dimensional array, but `ind`, your index array, is a $2 \\times 2$ array, and that is the shape the results comes back in.\n\n> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What happens when your index array is bigger than the target array?\n# Hint: you could use a large one-dimensional array or something fancier like ind = np.arange(0, 12).reshape((6, 2))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Fancy indexing also works in multiple dimensions:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "arr2 = np.arange(12).reshape((3, 4))\narr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As with standard indexing, the first index refers to the row and the second to the column:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "row = np.array([0, 1, 2])\ncol = np.array([2, 1, 3])\narr2[row, col]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What did you actually get as your final result here? The first value in the result array is `arr2[0, 2]`, the second one is `arr2[1, 1]`, and the third one is `arr2[2, 3]`.\n\nThe pairing of indices in fancy indexing follows all the same broadcasting rules we covered earlier. Thus, if you combine a column vector and a row vector within the indices, you get a two-dimensional result:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "arr2[row[:, np.newaxis], col]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here, each row value is matched with each column vector, exactly as we saw in broadcasting of arithmetic operations.\n\n> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now try broadcasting this on your own.\n# What do you get with row[:, np.newaxis] * col? \n# Or row[:, np.newaxis] * row? col[:, np.newaxis] * row?\n# What about col[:, np.newaxis] * row?\n# Hint: think back to the broadcast rules\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**The big takeaway:** It is always important to remember that fancy indexing returns values reflected by the *broadcasted shape of the indices*, and not the shape of the array being indexed."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Combined indexing\n\nYou can also combine fancy indexing with the other indexing schemes you have learned. Consider `arr2` again:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(arr2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now combine fancy and simple indices:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "arr2[2, [2, 0, 1]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What did you get back? The elements at positions 2, 0, and 1 of row 2 (the third row).\n\nYou can also combine fancy indexing with slicing:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "arr2[1:, [2, 0, 1]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, consider what you got back as output: the elements at positions 2, 0, and 1 of each row after the first one (so the second and third rows).\n\nOf course, you can also combine fancy indexing with masking:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "mask = np.array([1, 0, 1, 0], dtype=bool)\narr2[row[:, np.newaxis], mask]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Modifying values using fancy indexing\n\nFancy indexing is, of course, not just for accessing parts of an array, but also for modifying parts of an array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind = np.arange(10)\narr = np.array([2, 1, 8, 4])\nind[arr] = 99\nprint(ind)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also use a ufunc here and subtract 10 from each element of the array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind[arr] -= 10\nprint(ind)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Be cautious when using repeated indices with operations like these. They might not always produce the results you expect. For example:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind = np.zeros(10)\nind[[0, 0]] = [4, 6]\nprint(ind)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Where did the 4 go? The result of this operation is to first assign `ind[0] = 4`, followed by `ind[0] = 6`. So the result is that `ind[0]` contains the value 6.\n\nBut not every operation repeats the way you might think it should:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "arr = [2, 3, 3, 4, 4, 4]\nind[arr] += 1\nind",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We might have expected that `ind[3]` would contain the value 2 and `ind[4]` would contain the value 3. After all, that is how many times each index is repeated. So what happened?\n\nThis happened because `ind[arr] += 1` is really shorthand for `ind[arr] = ind[arr] + 1`. `ind[arr] + 1` is evaluated, and then the result is assigned to the indices in `ind`. So, similar to the previous example, this is not augmentation that happens multiple times, but an assignment, which can lead to potentially counterintuitive results.\n\nBut what if you want an operation to repeat? To do this, use the `at()` method of ufuncs:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind = np.zeros(10)\nnp.add.at(ind, arr, 1)\nprint(ind)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What does np.subtract.at(ind, arr, 1) give you?\n# Play around with some of the other ufuncs we have seen.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Fancy indexing enables you to select and manipulate several array members at once. This type of programmatic data manipulation is common in data science: often what you want to do with your data you want to do on several data points at once."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Sorting arrays\n\nSo far we have just worried about accessing and modifying NumPy arrays. Another huge thing you will need to do as a data scientist is sort array data. Sorting is often an important means of teasing out the structure in data (such as outlying data points).\n\nAlthough you could use Python's built-in `sort` and `sorted` functions, they will not work nearly as efficiently as NumPy's `np.sort` function.\n\n`np.sort` returns a sorted version of an array without modifying the input:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = np.array([2, 1, 4, 3, 5])\nnp.sort(a)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In order to sort the array in-place, use the `sort` method directly on arrays:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a.sort()\nprint(a)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A related function is `argsort`, which returns the *indices* of the sorted elements rather than the elements themselves:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = np.array([2, 1, 4, 3, 5])\nb = np.argsort(a)\nprint(b)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The first element of this result gives the index of the smallest element, the second value gives the index of the second smallest, and so on. These indices can then be used (via fancy indexing) to reconstruct the sorted array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a[b]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Sorting along rows or columns\n\nA useful feature of NumPy's sorting algorithms is the ability to sort along specific rows or columns of a multidimensional array using the `axis` argument. For example:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "rand = np.random.RandomState(42)\ntable = rand.randint(0, 10, (4, 6))\nprint(table)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Sort each column of the table\nnp.sort(table, axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Sort each row of the table\nnp.sort(table, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Bear in mind that this treats each row or column as an independent array; any relationships between the row or column values will be lost doing this kind of sorting."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Partial sorting: partitioning\n\nSometimes you don't need to sort an entire array, you just need to find the *k* smallest values in the array (often when looking at the distance of data points from one another). NumPy supplies this functionality through the `np.partition` function. `np.partition` takes an array and a number *k*; the result is a new array with the smallest *k* values to the left of the partition, and the remaining values to the right (in arbitrary order):"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "arr = np.array([7, 2, 3, 1, 6, 5, 4])\nnp.partition(arr, 3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that the first three values in the resulting array are the three smallest in the array, and the remaining array positions contain the remaining values. Within the two partitions, the elements have arbitrary order.\n\nSimilarly to sorting, we can partition along an arbitrary axis of a multidimensional array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.partition(table, 2, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The result is an array where the first two slots in each row contain the smallest values from that row, with the remaining values filling the remaining slots.\n\nFinally, just as there is an `np.argsort` that computes indices of the sort, there is an `np.argpartition` that computes indices of the partition. We'll see this in action in the following section when we discuss pandas.\n\n> **Takeaway:** Sorting your data is a fundamental means of exploring it and answering questions about it. The sorting algorithms in NumPy provide you with a fast, computationally efficient way of doing this on large amounts of data and with fine-grain control."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Efficient computation on NumPy arrays: Universal functions\n\n> **Learning goal:** By the end of this subsection, you should have a basic understanding of what NumPy universal functions are and how (and why) to use them.\n\nSome of the properties that make Python great to work with for data science (its dynamic, interpreted nature, for example) can also make it slow. This is particularly true with looping. These small performance hits can add up to minutes (or longer) when dealing with truly huge datasets.\n\nWhen we first examined loops in the previous section, you probably didn't notice any delay: the loops were short enough that Python’s relatively slow looping wasn’t an issue. Consider this function, which calculates the reciprocal for an array of numbers:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nnp.random.seed(0)\n\ndef compute_reciprocals(values):\n    output = np.empty(len(values))\n    for i in range(len(values)):\n        output[i] = 1.0 / values[i]\n    return output\n        \nvalues = np.random.randint(1, 10, size=5)\ncompute_reciprocals(values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Running this loop, it was probably difficult to even discern that execution wasn't instantaneous.\n\nBut let’s try it on a much larger array. To empirically do this, we'll time this with IPython's `%timeit` magic command."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "big_array = np.random.randint(1, 100, size=1000000)\n%timeit compute_reciprocals(big_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You certainly noticed that delay. The slowness of this looping becomes noticeable when we repeat many small operations many times.\n\nThe performance bottleneck is not the operations themselves, but the type-checking and function dispatches that Python performs on each cycle of the loop. In the case of the `compute_reciprocals` function above, each time Python computes the reciprocal, it first examines the object's type and does a dynamic lookup of the correct function to use for that type. Such is life with interpreted code. However, were we working with compiled code instead (such as in C), the object-type specification would be known before the code executes, and the result could be computed much more efficiently. This is where NumPy universal functions come into play."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Ufuncs\n\nUniversal functions in NumPy (often shortened to *ufuncs*) provide a statically typed, compiled function for many of the operations that we will need to run in the course of manipulating and analyzing data.\n\nLet's examine what this means in practice. Let's find the reciprocals of `big_array` again, this time using a built-in NumPy division ufunc on the array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "%timeit (1.0 / big_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That’s orders of magnitude better.\n\nUfuncs can be used between a scalar and an array and between arrays of arbitrary dimensions.\n\nComputations vectorized by ufuncs are almost always more efficient than doing the same computation using Python loops. This is especially true on large arrays. When possible, try to use ufuncs when operating on NumPy arrays, rather than using ordinary Python loops.\n\nUfuncs come in two flavors: *unary ufuncs*, which use a single input, and *binary ufuncs*, which operate on two inputs. The common ufuncs we'll look at here encompass both kinds."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Array arithmetic\n\nMany NumPy ufuncs use Python's native arithmetic operators, so you can use the standard addition, subtraction, multiplication, and division operators that we covered in the first section:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = np.arange(4)\nprint(\"a     =\", a)\nprint(\"a + 5 =\", a + 5)\nprint(\"a - 5 =\", a - 5)\nprint(\"a * 2 =\", a * 2)\nprint(\"a / 2 =\", a / 2)\nprint(\"a // 2 =\", a // 2)  # floor division",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are also ufuncs for negation, exponentiation, and the modulo operation:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"-a     = \", -a)\nprint(\"a ** 2 = \", a ** 2)\nprint(\"a % 2  = \", a % 2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also combine these ufuncs using the standard order of operations:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "-(0.5*a + 1) ** 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The Python operators are not actually the ufuncs, but are rather wrappers around functions built into NumPy. So the `+` operator is actually a wrapper for the `add` function:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.add(a, 2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here is a cheat sheet for the equivalencies between Python operators and NumPy ufuncs:\n\n| Operator\t    | Equivalent ufunc    | Description                           |\n|:--------------|:--------------------|:--------------------------------------|\n|``+``          |``np.add``           |Addition (e.g., ``1 + 1 = 2``)         |\n|``-``          |``np.subtract``      |Subtraction (e.g., ``3 - 2 = 1``)      |\n|``-``          |``np.negative``      |Unary negation (e.g., ``-2``)          |\n|``*``          |``np.multiply``      |Multiplication (e.g., ``2 * 3 = 6``)   |\n|``/``          |``np.divide``        |Division (e.g., ``3 / 2 = 1.5``)       |\n|``//``         |``np.floor_divide``  |Floor division (e.g., ``3 // 2 = 1``)  |\n|``**``         |``np.power``         |Exponentiation (e.g., ``2 ** 3 = 8``)  |\n|``%``          |``np.mod``           |Modulus/remainder (e.g., ``9 % 4 = 1``)|\n\nPython Boolean operators also work; we will explore those later in this section."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Absolute value\n\nNumPy also understands Python's built-in absolute value function:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = np.array([-2, -1, 0, 1, 2])\nabs(a)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This corresponds to the NumPy ufunc `np.absolute` (which is also available under the alias `np.abs`):"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.absolute(a)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.abs(a)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Exponents and logarithms\n\nYou will need to use exponents and logarithms a lot in data science; these are some of the most common data transformations for machine learning and statistical work."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = [1, 2, 3]\nprint(\"a     =\", a)\nprint(\"e^a   =\", np.exp(a))\nprint(\"2^a   =\", np.exp2(a))\nprint(\"3^a   =\", np.power(3, a))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The basic `np.log` gives the natural logarithm; if you need to compute base-2 or base-10 logarithms, NumPy also provides those:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = [1, 2, 4, 10]\nprint(\"a        =\", a)\nprint(\"ln(a)    =\", np.log(a))\nprint(\"log2(a)  =\", np.log2(a))\nprint(\"log10(a) =\", np.log10(a))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are also some specialized versions of these ufuncs to help maintain precision when dealing with very small inputs:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "a = [0, 0.001, 0.01, 0.1]\nprint(\"exp(a) - 1 =\", np.expm1(a))\nprint(\"log(1 + a) =\", np.log1p(a))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These functions give more precise values than if you were to use the raw `np.log` or `np.exp` on very small values of `a`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Specialized ufuncs\n\nNumPy has many other ufuncs. Another source for specialized and obscure ufuncs is the submodule `scipy.special`. If you need to compute some specialized mathematical or statistical function on your data, chances are it is implemented in `scipy.special`."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from scipy import special",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Gamma functions (generalized factorials) and related functions\na = [1, 5, 10]\nprint(\"gamma(a)     =\", special.gamma(a))\nprint(\"ln|gamma(a)| =\", special.gammaln(a))\nprint(\"beta(a, 2)   =\", special.beta(a, 2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Universal functions in NumPy provide you with computational functions that are faster than regular Python functions, particularly when working on large datasets that are common in data science. This speed is important because it can make you more efficient as a data scientist and it makes a broader range of inquiries into your data tractable in terms of time and computational resources."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Aggregations\n\n> **Learning goal:** By the end of this subsection, you should be comfortable aggregating data in NumPy.\n\nOne of the first things you will find yourself doing with most datasets is computing the summary statistics for the data in order to get a general overview of your data before exploring it further. These summary statistics include the mean and standard deviation, in addition to other aggregates, such as the sum, product, median, minimum and maximum, or quantiles of the data.\n\nNumPy has fast built-in aggregation functions for working on arrays that are the subject of this sub-section."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Summing the values of an array\n\nYou can use the built-in Python `sum` function to sum up the values in an array."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "myList = np.random.random(100)\nsum(myList)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If you guessed that there is also a built-in NumPy function for this, you guessed correctly:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.sum(myList)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And if you guessed that the NumPy version is faster, you are doubly correct:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "large_array = np.random.rand(1000000)\n%timeit sum(large_array)\n%timeit np.sum(large_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "For all their similarity, bear in mind that `sum` and `np.sum` are not identical; their optional arguments have different meanings, and `np.sum` is aware of multiple array dimensions."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Minimum and maximum\n\nJust as Python has built-in `min` and `max` functions, NumPy has similar, vectorized versions:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.min(large_array), np.max(large_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also use `min`, `max`, and `sum` (and several other NumPy aggregates) as methods of the array object itself:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(large_array.min(), large_array.max(), large_array.sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Multidimensional aggregates\n\nBecause you will often treat the rows and columns of two-dimensional arrays differently (treating columns as variables and rows as observations of those variables, for example), it can often be desirable to aggregate array data along a row or column. Let's consider a two-dimensional array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "md = np.random.random((3, 4))\nprint(md)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Unless you specify otherwise, each NumPy aggregation function will compute the aggregate for the entire array. Hence:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "md.sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Aggregation functions take an additional argument specifying the *axis* along which to compute the aggregation. For example, we can find the minimum value within each column by specifying `axis=0`:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "md.min(axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What do you get when you try md.max(axis=1)?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember that the `axis` keyword specifies the *dimension of the array that is to be collapsed*, not the dimension that will be returned. Thus specifying `axis=0` means that the first axis will be the one collapsed: for two-dimensional arrays, this means that values within each column will be aggregated."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Other aggregation functions\n\nThe table below lists other aggregation functions in NumPy. Most NumPy aggregates have a '`NaN`-safe' version, which computes the result while ignoring missing values marked by the `NaN` value.\n\n|Function Name      |   NaN-safe Version  | Description                                   |\n|:------------------|:--------------------|:----------------------------------------------|\n| ``np.sum``        | ``np.nansum``       | Compute sum of elements                       |\n| ``np.prod``       | ``np.nanprod``      | Compute product of elements                   |\n| ``np.mean``       | ``np.nanmean``      | Compute mean of elements                      |\n| ``np.std``        | ``np.nanstd``       | Compute standard deviation                    |\n| ``np.var``        | ``np.nanvar``       | Compute variance                              |\n| ``np.min``        | ``np.nanmin``       | Find minimum value                            |\n| ``np.max``        | ``np.nanmax``       | Find maximum value                            |\n| ``np.argmin``     | ``np.nanargmin``    | Find index of minimum value                   |\n| ``np.argmax``     | ``np.nanargmax``    | Find index of maximum value                   |\n| ``np.median``     | ``np.nanmedian``    | Compute median of elements                    |\n| ``np.percentile`` | ``np.nanpercentile``| Compute rank-based statistics of elements     |\n| ``np.any``        | N/A                 | Evaluate whether any elements are true        |\n| ``np.all``        | N/A                 | Evaluate whether all elements are true        |\n\nWe will see these aggregates often throughout the rest of the course."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Aggregation is the primary means you will use to explore you data, not just when using NumPy, but particularly in conjunction with pandas, the Python library you will learn about in the next section, which builds off of NumPy and thus off of everything you have learned thus far."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Computation on arrays with broadcasting\n\n> **Learning goal:** By the end of this subsection, you should have a basic understanding of how broadcasting works in NumPy (and why NumPy uses it).\n\nAnother means of vectorizing operations is to use NumPy's *broadcasting* functionality: creating rules for applying binary ufuncs like addition, subtraction, or multiplication on arrays of different sizes.\n\nBefore, when we performed binary operations on arrays of the same size, those operations were performed on an element-by-element basis."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "first_array = np.array([3, 6, 8, 1])\nsecond_array = np.array([4, 5, 7, 2])\nfirst_array + second_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Broadcasting enables you to perform these types of binary operations on arrays of different sizes. Thus, you could just as easily add a scalar (which is really just a zero-dimensional array) to an array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "first_array + 5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similarly, you can add a one-dimensional array to a two-dimensional array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "one_dim_array = np.ones((1))\none_dim_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "two_dim_array = np.ones((2, 2))\ntwo_dim_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "one_dim_array + two_dim_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So far, so easy. But you can use broadcasting on arrays in more complicated ways. Consider this example:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "horizontal_array = np.arange(3)\nvertical_array = np.arange(3)[:, np.newaxis]\n\nprint(horizontal_array)\nprint(vertical_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "horizontal_array + vertical_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Rules of broadcasting\nBroadcasting ollows a set of rules to determine the interaction between the two arrays:\n- **Rule 1**: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is *padded* with ones on its leading (left) side.\n- **Rule 2**: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape.\n- **Rule 3**: If, in any dimension, the sizes disagree and neither is equal to 1, NumPy raises an error.\n\nLet's see these rules in action to better understand them."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Broadcasting example 1\n\nLet's look at adding a two-dimensional array to a one-dimensional array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "two_dim_array = np.ones((2, 3))\none_dim_array = np.arange(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's consider an operation on these two arrays. The shape of the arrays are:\n\n- `two_dim_array.shape = (2, 3)`\n- `one_dim_array.shape = (3,)`\n\nWe see by rule 1 that the array `one_dim_array` has fewer dimensions, so we pad it on the left with ones:\n\n- `two_dim_array.shape -> (2, 3)`\n- `one_dim_array.shape -> (1, 3)`\n\nBy rule 2, we now see that the first dimension disagrees, so we stretch this dimension to match:\n\n- `two_dim_array.shape -> (2, 3)`\n- `one_dim_array.shape -> (2, 3)`\n\nThe shapes match, and we see that the final shape will be `(2, 3)`:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "two_dim_array + one_dim_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Flip this around. Try adding these with two_dim_array = np.ones((3, 2)) \n# and one_dim_array = np.arange(3)[:, np.newaxis].\n# What do you get?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Broadcasting example 2\n\nLet's examine what happens when both arrays need to be broadcast:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "vertical_array = np.arange(3).reshape((3, 1))\nhorizontal_array = np.arange(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, we'll start by writing out the shape of the arrays:\n\n- `vertical_array.shape = (3, 1)`\n- `horizontal_array.shape = (3,)`\n\nRule 1 says we must pad the shape of `horizontal_array ` with ones:\n\n- `vertical_array.shape -> (3, 1)`\n- `horizontal_array.shape -> (1, 3)`\n\nAnd rule 2 tells us that we upgrade each of these ones to match the corresponding size of the other array:\n\n- `vertical_array.shape -> (3, 3)`\n- `horizontal_array.shape -> (3, 3)`\n\nBecause the result matches, these shapes are compatible. We can see this here:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "vertical_array + horizontal_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Broadcasting example 3\n\nHere's what happens with incompatible arrays:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "M = np.ones((3, 2))\ni = np.arange(3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is just a slightly different situation than in the first example: the matrix ``M`` is transposed.\nHow does this affect the calculation? The shape of the arrays are:\n\n- ``M.shape = (3, 2)``\n- ``i.shape = (3,)``\n\nAgain, rule 1 tells us that we must pad the shape of ``i`` with ones:\n\n- ``M.shape -> (3, 2)``\n- ``i.shape -> (1, 3)``\n\nBy rule 2, the first dimension of ``i`` is stretched to match that of ``M``:\n\n- ``M.shape -> (3, 2)``\n- ``i.shape -> (3, 3)``\n\nNow we hit Rule 3: the final shapes do not match and the two arrays are incompatible:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "M + i",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Broadcasting in practice\nUfuncs enable you to avoid using slow Python loops; broadcasting builds on that.\n\nA common data practice is to *center* an array of data. For example, if we have an array of 10 observations, each of which consists of three values (called features in this context), we might want to center that data so that we have the differences from the mean rather than the raw data itself. Doing this can help us better compare the different values.\n\nWe'll store this in a $10 \\times 3$ array:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "T = np.random.random((10, 3))\nT",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now compute the mean of each feature using the ``mean`` aggregate across the first dimension:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "Tmean = T.mean(0)\nTmean",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, center ``T`` by subtracting the mean. (This is a broadcasting operation.)"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "T_centered = T - Tmean\nT_centered",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This is not just faster, but easier than writing a loop to do this."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** The data you will work with in data science invariably comes in different shapes and sizes (at least in terms of the arrays in which you work with that data). The broadcasting functionality in NumPy enables you to use binary functions on irregularly fitting data in a predictable way."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Comparisons, masks, and Boolean logic in NumPy\n\n> **Learning goal:** By the end of this subsection, you should be comfortable with and understand how to use Boolean masking in NumPy in order to answer basic questions about your data.\n\n*Masking* is when you want to manipulate or count or extract values in an array based on a criterion. For example, counting all the values in an array greater than a certain value is an example of masking. Boolean masking is often the most efficient way to accomplish these types of tasks in NumPy and it plays a large part in cleaning and otherwise preparing data for analysis (which we will cover in a later section)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Example: Counting Rainy Days\n\nLet's see masking in practice by examining the monthly rainfall statistics for Seattle. The data is in a CSV file from data.gov. To load the data, we will use pandas, which we will formally introduce later."
    },
    {
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\n\n# Use pandas to extract rainfall as a NumPy array\nrainfall_2003 = pd.read_csv('Data/Observed_Monthly_Rain_Gauge_Accumulations_-_Oct_2002_to_May_2017.csv')['RG01'][ 2:14].values\nrainfall_2003",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let’s break down what we just did in the code cell above. The rainfall data contains monthly rainfall totals from several rain gauges around the city of Seattle; we selected the first one. From that gauge, we then selected the relevant months for the first full calendar year in the dataset, 2003. That range of months started at the third row of the CSV file (remember, Python zero-indexes!) and ran through the thirteenth row, hence `2:14]`.\n\nYou now have an array containing 12 values, each of which records the monthly rainfall in inches from January to December 2003.\n\nCommonly in data science, you will want to take a quick first exploratory look at the data. In this case, a bar chart is a good way to do this. To generate this bar chart, we will use Matplotlib, another important data-science tool that we will introduce formally later in the course. (This also brings up another widely used Python convention you should adopt: `import matplotlib.pyplot as plt`.)"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib.pyplot as plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.bar(np.arange(1, len(rainfall_2003) + 1), rainfall_2003)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To briefly interpret the code snippet above, we passed two parameters to the bar function in pyplot: the first defining the index for the x-axis and the second defining the data to use for the bars (the y-axis). To create the index, we use the NumPy function `arange` to create a sequence of numbers (this is the same `arange` we encountered earlier in this section). We know that the length of our array is 12, but it can be a good habit to get into to programmatically pass the length of an array in case it changes or you don’t know it with specificity. We also added 1 to both the start and the end of the `arange` to accommodate for Python zero-indexing (because there is no “month-zero” in the calendar).\n\nLooking at the chart above (and as residents can attest), Seattle can have lovely, sunny summers. However, this is only a first glimpse of the data. There are still several questions we would like to answer, such as in how many months did it rain, or what was the average precipitation in those months? We would use masking to answer those questions. (We will also return to this example dataset to demonstrate concepts throughout the rest of this section.) Before we dive deeper in explaining what masking is, we should briefly touch on comparison operators in NumPy."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Comparison operators as ufuncs\n\nIn addition to the computational operators as ufuncs that we have already encountered, NumPy also implements comparison operators such as `<` (less than) and `>` (greater than) as element-wise ufuncs. All of the standard Python comparison operations are available:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "simple_array = np.array([1, 2, 3, 4, 5])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "simple_array < 2  # less than",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "simple_array >= 4  # greater than or equal",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "simple_array == 2  # equal",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It is also possible to do an element-wise comparison of two arrays, and to include compound expressions:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "(2 * simple_array) == (simple_array ** 2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As with the arithmetic operators, these comparison operators are wrappers for the NumPy ufuncs: when you write ``x < 3``, NumPy actually uses ``np.less(x, 3)``. Here is a summary of the comparison operators and their equivalent ufuncs:\n\n| Operator\t    | Equivalent ufunc    || Operator\t   | Equivalent ufunc    |\n|:--------------|:--------------------||:--------------|:--------------------|\n|``==``         |``np.equal``         ||``!=``         |``np.not_equal``     |\n|``<``          |``np.less``          ||``<=``         |``np.less_equal``    |\n|``>``          |``np.greater``       ||``>=``         |``np.greater_equal`` |"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Just like the arithmetic ufuncs, the comparison ufuncs work on arrays of any size and shape."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "rand = np.random.RandomState(0)\ntwo_dim_array = rand.randint(10, size=(3, 4))\ntwo_dim_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "two_dim_array < 6",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The result is a Boolean array, and NumPy provides a number of straightforward patterns for working with these Boolean results."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Working with Boolean arrays\n\nGiven a Boolean array, there are a host of useful operations you can do.\nWe'll work with `two_dim_array`, the two-dimensional array we created earlier."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(two_dim_array)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Counting entries\n\nTo count the number of ``True`` entries in a Boolean array, ``np.count_nonzero`` is useful:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# how many values less than 6?\nnp.count_nonzero(two_dim_array < 6)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We see that there are eight array entries that are less than 6.\nAnother way to get at this information is to use ``np.sum``; in this case, ``False`` is interpreted as ``0``, and ``True`` is interpreted as ``1``:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.sum(two_dim_array < 5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The benefit of `sum()` is that, like with other NumPy aggregation functions, this summation can be done along rows or columns as well:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# how many values less than 5 in each row?\nnp.sum(two_dim_array < 5, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This counts the number of values less than 5 in each row of the matrix.\n\nIf we're interested in quickly checking whether any or all the values are true, we can use (you guessed it) ``np.any`` or ``np.all``:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Are there any values less than zero?\nnp.any(two_dim_array < 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now check to see if all values less than 10?\n# Hint: use np.all()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "``np.all`` and ``np.any`` can be used along particular axes as well. For example:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# are all values in each row less than 7?\nnp.all(two_dim_array < 7, axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here, all the elements in the first and third rows are less than 7, while this is not the case for the second row.\n\n**A reminder:** Python has built-in `sum()`, `any()`, and `all()` functions. These have a different syntax than the NumPy versions, and, in particular, will fail or produce unintended results when used on multidimensional arrays. Be sure that you are using `np.sum()`, `np.any()`, and `np.all()` for these examples."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Boolean operators\n\nWe've already seen how we might count, say, all months with rain less than four inches, or all months with more than two inches of rain. But what if we want to know about all months with rain less than four inches and greater than one inch? This is accomplished through Python's *bitwise logic operators*, `&`, `|`, `^`, and `~`. Like with the standard arithmetic operators, NumPy overloads these as ufuncs which work element-wise on (usually Boolean) arrays.\n\nFor example, we can address this sort of compound question as follows:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.sum((rainfall_2003 > 0.5) & (rainfall_2003 < 1))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So we see that there are two months with rainfall between 0.5 and 1.0 inches.\nNote that the parentheses here are important—because of operator-precedence rules, with parentheses removed, this expression would be evaluated as follows, which results in an error:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "rainfall_2003 > (0.5 & rainfall_2003) < 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Using the equivalence of *A AND B and NOT (NOT A OR NOT B)* (which you might remember if you've taken an introductory logic course), we can compute the same result in a different manner:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.sum(~((rainfall_2003 <= 0.5) | (rainfall_2003 >= 1)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Combining comparison operators and Boolean operators on arrays can lead to a wide range of efficient logical operations.\n\nThe following table summarizes the bitwise Boolean operators and their equivalent ufuncs:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "| Operator\t    | Equivalent ufunc    || Operator\t   | Equivalent ufunc    |\n|:--------------|:--------------------||:--------------|:--------------------|\n|``&``          |``np.bitwise_and``   ||&#124;         |``np.bitwise_or``    |\n|``^``          |``np.bitwise_xor``   ||``~``          |``np.bitwise_not``   |"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Using these tools, you can start to answer the types of questions we listed above about the Seattle rainfall data. Here are some examples of results we can compute when combining masking with aggregations:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(\"Number of months without rain:\", np.sum(rainfall_2003 == 0))\nprint(\"Number of months with rain:   \", np.sum(rainfall_2003 != 0))\nprint(\"Months with more than 1 inch: \", np.sum(rainfall_2003 > 1))\nprint(\"Rainy months with < 1 inch:   \", np.sum((rainfall_2003 > 0) &\n                                              (rainfall_2003 < 1)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Boolean arrays as masks\n\nIn the prior section, we looked at aggregates computed directly on Boolean arrays.\nA more powerful pattern is to use Boolean arrays as masks, to select particular subsets of the data themselves.\nReturning to our `two_dim_array` array from before, suppose we want an array of all values in the array that are less than 5:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "two_dim_array",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can obtain a Boolean array for this condition easily:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "two_dim_array < 5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, to *select* these values from the array, you can simply index on this Boolean array. This is the *masking* operation:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "two_dim_array[two_dim_array < 5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What is returned is a one-dimensional array filled with all the values that meet your condition. Put another way, these are all the values in positions at which the mask array is ``True``.\n\nYou can use masking as a way to compute some relevant statistics on the Seattle rain data:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Construct a mask of all rainy months\nrainy = (rainfall_2003 > 0)\n\n# Construct a mask of all summer months (June through September)\nmonths = np.arange(1, 13)\nsummer = (months > 5) & (months < 10)\n\nprint(\"Median precip in rainy months in 2003 (inches):   \", \n      np.median(rainfall_2003[rainy]))\nprint(\"Median precip in summer months in 2003 (inches):  \", \n      np.median(rainfall_2003[summer]))\nprint(\"Maximum precip in summer months in 2003 (inches): \", \n      np.max(rainfall_2003[summer]))\nprint(\"Median precip in non-summer rainy months (inches):\", \n      np.median(rainfall_2003[rainy & ~summer]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** By combining Boolean operations, masking operations, and aggregates, you can quickly answer questions similar to those we posed about the Seattle rainfall data about any dataset. Operations like these will form the basis for the data exploration and preparation for analysis that will by our primary concerns in the following sections."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 4: Pandas\n\nHaving explored NumPy in a previous section, it is time to get to know the other workhorse of data science in Python: pandas. The pandas library in Python really does a lot to make working with data--and importing, cleaning, and organizing it--so much easier that it is hard to imagine doing data science in Python without it.\n\nBut it was not always this way. Wes McKinney developed the library out of necessity in 2008 while at AQR Capital Management in order to have a better tool for dealing with data analysis. The library has since taken off as an open-source software project that has become a mature and integral part of the data science ecosystem. (In fact, some examples in this section will be drawn from McKinney's book, *Python for Data Analysis*.)\n\nThe name 'pandas' actually has nothing to do with Chinese bears but rather comes from the term *panel data*, a form of multi-dimensional data involving measurements over time that comes out the econometrics and statistics community. Ironically, while panel data is a usable data structure in pandas, it is not generally used today and we will not examine it in this course. Instead, we will focus on the two most widely used data structures in pandas: `Series` and `DataFrame`s."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Reminders about importing and documentation\n\nJust as you imported NumPy undwither the alias ``np``, we will import Pandas under the alias ``pd``:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As with the NumPy convention, `pd` is an important and widely used convention in the data science world; we will use it here and we advise you to use it in your own coding.\n\nAs we progress through the next section, don't forget that IPython provides tab-completion feature and function documentation with the ``?`` character. If you don't understand anything about a function you see in this section, take a moment and read the documentation; it can help a great deal. As a reminder, to display the built-in pandas documentation, use this code:\n\n```ipython\nIn [4]: pd?\n```\n\nBecause it can be useful to lean about `Series` and `DataFrame`s in pandas a extension of `ndarray`s in NumPy, go ahead also import NumPy; you will want it for some of the examples later on:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, on to pandas!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Fundamental panda data structures\n\nBoth `Series` and `DataFrame`s are a lot like they `ndarray`s you encountered in the last section. They provide clean, efficent data storage and handling at the scales necessary for data science. What both of them provide that `ndarray`s lack, however, are essential data-science features like flexibility when dealing with missing data and the ability to label data. These capabilities (along with others) help make `Series` and `DataFrame`s essential to the \"data munging\" that make up so much of data science."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `Series` objects in pandas\n\nA pandas `Series` is a lot like an `ndarray` in NumPy: a one-dimensional array of indexed data.\nYou can create a simple Series from an array of data like this:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example = pd.Series([-0.5, 0.75, 1.0, -2])\nseries_example",
      "execution_count": 3,
      "outputs": [
        {
          "data": {
            "text/plain": "0   -0.50\n1    0.75\n2    1.00\n3   -2.00\ndtype: float64"
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similar to an `ndarray`, a `Series` upcasts entries to be of the same type of data (that `-2` integer in the original array became a `-2.00` float in the `Series`).\n\nWhat is different from an `ndarray` is that the ``Series`` automatically wraps both a sequence of values and a sequence of indices. These are two separate objects within the `Seriers` object that can access with the ``values`` and ``index`` attributes.\n\nTry accessing the ``values`` first; they are just a familiar NumPy array:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example.values",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "array([-0.5 ,  0.75,  1.  , -2.  ])"
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The ``index`` is also an array-like object:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example.index",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "RangeIndex(start=0, stop=4, step=1)"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Just as with `ndarra`s, you can access specific data elements in a `Series` via the familiar Python square-bracket index notation and slicing:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example[1]",
      "execution_count": 7,
      "outputs": [
        {
          "data": {
            "text/plain": "0.75"
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example[1:3]",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "1    0.75\n2    1.00\ndtype: float64"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Despite a lot of similarities, pandas `Series` have an important distinction from NumPy `ndarrays`: whereas `ndarrays` have  *implicitly defined* integer indices (as do Python lists), pandas `Series` have *explicitly defined* indices. The best part is that you can set the index:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2 = pd.Series([-0.5, 0.75, 1.0, -2], index=['a', 'b', 'c', 'd'])\nseries_example2",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\nc    1.00\nd   -2.00\ndtype: float64"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These explicit indices work exactly the way you would expect them to:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2['b']",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "0.75"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Do explicit Series indices work *exactly* the way you might expect?\n# Try slicing series_example2 using its explicit index and find out.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With explicit indices in the mix, a `Series` is basically a fixed-length, ordered dictionary in that it maps arbitrary typed index values to arbitrary typed data values. But like `ndarray`s these data are all of the same type, which is important. Just as the type-specific compiled code behind `ndarray` makes them more efficient than a Python lists for certain operations, the type information of pandas ``Series`` makes them much more efficient than Python dictionaries for certain operations.\n\nBut the connection between `Series` and dictionaries is nevertheless very real: you can construct a ``Series`` object directly from a Python dictionary:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "population_dict = {'France': 65429495,\n                   'Germany': 82408706,\n                   'Russia': 143910127,\n                   'Japan': 126922333}\npopulation = pd.Series(population_dict)\npopulation",
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": "France      65429495\nGermany     82408706\nJapan      126922333\nRussia     143910127\ndtype: int64"
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did you see what happened there? The order of the keys `Russia` and `Japan` in the switched places between the order in which they were entered in `population_dict` and how they ended up in the `population` `Series` object. While Python dictionary keys have no order, `Series` keys are ordered.\n\nSo, at one level, you can interact with `Series` as you would with dictionaries:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "population['Russia']",
      "execution_count": 14,
      "outputs": [
        {
          "data": {
            "text/plain": "143910127"
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "But you can also do powerful array-like operations with `Series` like slicing:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Try slicing on the population Series on your own.\n# Would slicing be possible if Series keys were not ordered?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also add elements to a `Series` the way that you would to an `ndarray`. Try it in the code cell below:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Try running population['Albania'] = 2937590 (or another country of your choice)\n# What order do the keys appear in when you run population? Is it what you expected?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Anoter useful `Series` feature (and definitely a difference from dictionaries) is that `Series` automatically aligns differently indexed data in arithmetic operations:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pop2 = pd.Series({'Spain': 46432074, 'France': 102321, 'Albania': 50532})\npopulation + pop2",
      "execution_count": 20,
      "outputs": [
        {
          "data": {
            "text/plain": "Albania     2988122.0\nFrance     65531816.0\nGermany           NaN\nJapan             NaN\nRussia            NaN\nSpain             NaN\ndtype: float64"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that in the case of Germany, Japan, Russia, and Spain (and Albania, depending on what you did in the previous exercise), the addition operation produced `NaN` (not a number) values. pandas does not treat missing values as `0`, but as `NaN` (and it can be helpful to think of arithmetic operations involving `NaN` as essentially `NaN`$ + x=$ `NaN`)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `DataFrame` object in pandas\n\nThe other crucial data structure in pandas to get to know for data science is the `DataFrame`.\nLike the ``Series`` object, ``DataFrame``s can be thought of either as generalizations of `ndarray`s (or as specializations of Python dictionaries).\n\nJust as a ``Series`` is like a one-dimensional array with flexible indices, a ``DataFrame`` is like a two-dimensional array with both flexible row indices and flexible column names. Essentially, a `DataFrame` represents a rectangular table of data and contains an ordered collection of labeled columns, each of which can be a different value type (`string`, `int`, `float`, etc.).\nThe DataFrame has both a row and column index; in this way you can think of it as a dictionary of `Series`, all of which share the same index.\n\nLet's take a look at how this works in practice. We will start by creating a `Series` called `area`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "area_dict = {'Albania': 28748,\n             'France': 643801,\n             'Germany': 357386,\n             'Japan': 377972,\n             'Russia': 17125200}\narea = pd.Series(area_dict)\narea",
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/plain": "Albania       28748\nFrance       643801\nGermany      357386\nJapan        377972\nRussia     17125200\ndtype: int64"
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you can combine this with the `population` `Series` you created earlier by using a dictionary to construct a single two-dimensional table containing data from both `Series`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries = pd.DataFrame({'Population': population, 'Area': area})\ncountries",
      "execution_count": 57,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>28748</td>\n      <td>2937590</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>377972</td>\n      <td>126922333</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>17125200</td>\n      <td>143910127</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "             Area  Population\nAlbania     28748     2937590\nFrance     643801    65429495\nGermany    357386    82408706\nJapan      377972   126922333\nRussia   17125200   143910127"
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As with `Series`, note that `DataFrame`s also automatically order indices (in this case, the column indices `Area` and `Population`).\n\nSo far we have combined dictionaries together to compose a `DataFrame` (which has given our `DataFrame` a row-centric feel), but you can also create `DataFrame`s in a column-wise fashion. Consider adding a `Capital` column using our reliable old array-analog, a list:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries['Capital'] = ['Tirana', 'Paris', 'Berlin', 'Tokyo', 'Moscow']\ncountries",
      "execution_count": 58,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Capital</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>28748</td>\n      <td>2937590</td>\n      <td>Tirana</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>Paris</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>Berlin</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>Tokyo</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>17125200</td>\n      <td>143910127</td>\n      <td>Moscow</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "             Area  Population Capital\nAlbania     28748     2937590  Tirana\nFrance     643801    65429495   Paris\nGermany    357386    82408706  Berlin\nJapan      377972   126922333   Tokyo\nRussia   17125200   143910127  Moscow"
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As with `Series`, even though initial indices are ordered in `DataFrame`s, subsequent additions to a `DataFrame` stay in the ordered added. However, you can explicitly change the order of `DataFrame` column indices this way:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries = countries[['Capital', 'Area', 'Population']]\ncountries",
      "execution_count": 59,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Capital</th>\n      <th>Area</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>Tirana</td>\n      <td>28748</td>\n      <td>2937590</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>Paris</td>\n      <td>643801</td>\n      <td>65429495</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>Berlin</td>\n      <td>357386</td>\n      <td>82408706</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>Tokyo</td>\n      <td>377972</td>\n      <td>126922333</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>Moscow</td>\n      <td>17125200</td>\n      <td>143910127</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        Capital      Area  Population\nAlbania  Tirana     28748     2937590\nFrance    Paris    643801    65429495\nGermany  Berlin    357386    82408706\nJapan     Tokyo    377972   126922333\nRussia   Moscow  17125200   143910127"
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Commonly in a data science context, it is necessary to generate new columns of data from existing data sets. Because `DataFrame` columns behave like `Series`, you can do this is by performing operations on them as you would with `Series`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries['Population Density'] = countries['Population'] / countries['Area']\ncountries",
      "execution_count": 60,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "C:\\Users\\jeppesen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"Entry point for launching an IPython kernel.\n"
        },
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Capital</th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>Tirana</td>\n      <td>28748</td>\n      <td>2937590</td>\n      <td>102.184152</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>Paris</td>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>101.629999</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>Berlin</td>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>Tokyo</td>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>335.798242</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>Moscow</td>\n      <td>17125200</td>\n      <td>143910127</td>\n      <td>8.403413</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        Capital      Area  Population  Population Density\nAlbania  Tirana     28748     2937590          102.184152\nFrance    Paris    643801    65429495          101.629999\nGermany  Berlin    357386    82408706          230.587393\nJapan     Tokyo    377972   126922333          335.798242\nRussia   Moscow  17125200   143910127            8.403413"
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note: don't worry if IPython gives you a warning over this. The warning is IPython trying to be a little too helpful. The new column you created is an actual part of the `DataFrame` and not a copy of a slice."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We have stated before that `DataFrame`s are like dictionaries, and it's true. You can retrieve the contents of a column just as you would the value for a specific key in an ordinary dictionary:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries['Area']",
      "execution_count": 40,
      "outputs": [
        {
          "data": {
            "text/plain": "Albania       28748\nFrance       643801\nGermany      357386\nJapan        377972\nRussia     17125200\nName: Area, dtype: int64"
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What about using the row indices?"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now try accessing row data with a command like countries['Japan']\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This returns an error: `DataFrame`s are dictionaries of `Series`, which are the columns. `DataFrame` rows often have heterogeneous data types, so different methods are necessary to access row data. For that, we use the `.loc` method:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries.loc['Japan']",
      "execution_count": 41,
      "outputs": [
        {
          "data": {
            "text/plain": "Capital                   Tokyo\nArea                     377972\nPopulation            126922333\nPopulation Density      335.798\nName: Japan, dtype: object"
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that what `.loc` returns is an indexed object in its own right and you can access elements within it using familiar index syntax:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries.loc['Japan']['Area']",
      "execution_count": 42,
      "outputs": [
        {
          "data": {
            "text/plain": "377972"
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Can you think of a way to return the area of Japan without using .iloc?\n# Hint: Try putting the column index first.\n# Can you slice along these indices as well?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Sometimes it is helpful in data science projects to add a column to a `DataFrame` without assigning values to it:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries['Debt-to-GDP Ratio'] = np.nan\ncountries",
      "execution_count": 61,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "C:\\Users\\jeppesen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"Entry point for launching an IPython kernel.\n"
        },
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Capital</th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n      <th>Debt-to-GDP Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>Tirana</td>\n      <td>28748</td>\n      <td>2937590</td>\n      <td>102.184152</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>Paris</td>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>101.629999</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>Berlin</td>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>Tokyo</td>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>335.798242</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>Moscow</td>\n      <td>17125200</td>\n      <td>143910127</td>\n      <td>8.403413</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        Capital      Area  Population  Population Density  Debt-to-GDP Ratio\nAlbania  Tirana     28748     2937590          102.184152                NaN\nFrance    Paris    643801    65429495          101.629999                NaN\nGermany  Berlin    357386    82408706          230.587393                NaN\nJapan     Tokyo    377972   126922333          335.798242                NaN\nRussia   Moscow  17125200   143910127            8.403413                NaN"
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, you can disregard the warning (if it triggers) about adding the column this way.\n\nYou can also add columns to a `DataFrame` that do not have the same number of rows as the `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "debt = pd.Series([0.19, 2.36], index=['Russia', 'Japan'])\ncountries['Debt-to-GDP Ratio'] = debt\ncountries",
      "execution_count": 62,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "C:\\Users\\jeppesen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n"
        },
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Capital</th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n      <th>Debt-to-GDP Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>Tirana</td>\n      <td>28748</td>\n      <td>2937590</td>\n      <td>102.184152</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>Paris</td>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>101.629999</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>Berlin</td>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>Tokyo</td>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>335.798242</td>\n      <td>2.36</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>Moscow</td>\n      <td>17125200</td>\n      <td>143910127</td>\n      <td>8.403413</td>\n      <td>0.19</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        Capital      Area  Population  Population Density  Debt-to-GDP Ratio\nAlbania  Tirana     28748     2937590          102.184152                NaN\nFrance    Paris    643801    65429495          101.629999                NaN\nGermany  Berlin    357386    82408706          230.587393                NaN\nJapan     Tokyo    377972   126922333          335.798242               2.36\nRussia   Moscow  17125200   143910127            8.403413               0.19"
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can use the `del` command to delete a column from a `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "del countries['Capital']\ncountries",
      "execution_count": 63,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n      <th>Debt-to-GDP Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>28748</td>\n      <td>2937590</td>\n      <td>102.184152</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>101.629999</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>335.798242</td>\n      <td>2.36</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>17125200</td>\n      <td>143910127</td>\n      <td>8.403413</td>\n      <td>0.19</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "             Area  Population  Population Density  Debt-to-GDP Ratio\nAlbania     28748     2937590          102.184152                NaN\nFrance     643801    65429495          101.629999                NaN\nGermany    357386    82408706          230.587393                NaN\nJapan      377972   126922333          335.798242               2.36\nRussia   17125200   143910127            8.403413               0.19"
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In addition to their dictionary-like behavior, `DataFrames` also behave like two-dimensional arrays. For example, it can be useful at times when working with a `DataFrame` to transpose it:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries.T",
      "execution_count": 64,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Albania</th>\n      <th>France</th>\n      <th>Germany</th>\n      <th>Japan</th>\n      <th>Russia</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Area</th>\n      <td>2.874800e+04</td>\n      <td>6.438010e+05</td>\n      <td>3.573860e+05</td>\n      <td>3.779720e+05</td>\n      <td>1.712520e+07</td>\n    </tr>\n    <tr>\n      <th>Population</th>\n      <td>2.937590e+06</td>\n      <td>6.542950e+07</td>\n      <td>8.240871e+07</td>\n      <td>1.269223e+08</td>\n      <td>1.439101e+08</td>\n    </tr>\n    <tr>\n      <th>Population Density</th>\n      <td>1.021842e+02</td>\n      <td>1.016300e+02</td>\n      <td>2.305874e+02</td>\n      <td>3.357982e+02</td>\n      <td>8.403413e+00</td>\n    </tr>\n    <tr>\n      <th>Debt-to-GDP Ratio</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.360000e+00</td>\n      <td>1.900000e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                         Albania        France       Germany         Japan  \\\nArea                2.874800e+04  6.438010e+05  3.573860e+05  3.779720e+05   \nPopulation          2.937590e+06  6.542950e+07  8.240871e+07  1.269223e+08   \nPopulation Density  1.021842e+02  1.016300e+02  2.305874e+02  3.357982e+02   \nDebt-to-GDP Ratio            NaN           NaN           NaN  2.360000e+00   \n\n                          Russia  \nArea                1.712520e+07  \nPopulation          1.439101e+08  \nPopulation Density  8.403413e+00  \nDebt-to-GDP Ratio   1.900000e-01  "
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, note that `DataFrame` columns are `Series` and thus the data types must consistent, hence the upcasting to floating-point numbers. **If there had been strings in this `DataFrame`, everything would have been upcast to strings.** Use caution when transposing `DataFrame`s."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### From a two-dimensional NumPy array\n\nGiven a two-dimensional array of data, we can create a ``DataFrame`` with any specified column and index names.\nIf omitted, an integer index will be used for each:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.DataFrame(np.random.rand(3, 2),\n             columns=['foo', 'bar'],\n             index=['a', 'b', 'c'])",
      "execution_count": 27,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>foo</th>\n      <th>bar</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>a</th>\n      <td>0.865257</td>\n      <td>0.213169</td>\n    </tr>\n    <tr>\n      <th>b</th>\n      <td>0.442759</td>\n      <td>0.108267</td>\n    </tr>\n    <tr>\n      <th>c</th>\n      <td>0.047110</td>\n      <td>0.905718</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "        foo       bar\na  0.865257  0.213169\nb  0.442759  0.108267\nc  0.047110  0.905718"
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Manipulating data in pandas\n\nA huge part of data science is manipulating data in order to analyze it. (One rule of thumb is that 80% of any data science project will be concerned with cleaning and organizing the data for the project.) So it makes sense to lear the tools that pandas provides for handling data in `Series` and especially `DataFrame`s. Because both of those data structures are ordered, let's first start by taking a closer look at what gives them their structure: the `Index`."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Index objects in pandas\n\nBoth ``Series`` and ``DataFrame``s in pandas have explicit indices that enable you to reference and modify data in them. These indices are actually objects themselves. The ``Index`` object can be thought of as both an immutable array or as fixed-size set. \n\nIt's worth the time to get to know the properties of the `Index` object. Let's return to an example from earlier in the section to examine these properties."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example = pd.Series([-0.5, 0.75, 1.0, -2], index=['a', 'b', 'c', 'd'])\nind = series_example.index\nind",
      "execution_count": 66,
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['a', 'b', 'c', 'd'], dtype='object')"
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The ``Index`` works a lot like an array. we have already seen how to use standard Python indexing notation to retrieve values or slices:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind[1]",
      "execution_count": 67,
      "outputs": [
        {
          "data": {
            "text/plain": "'b'"
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind[::2]",
      "execution_count": 68,
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['a', 'c'], dtype='object')"
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "But ``Index`` objects are immutable; you cannot be modified via the normal means:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind[1] = 0",
      "execution_count": 69,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Index does not support mutable operations",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-69-40e631c82e8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mC:\\Users\\jeppesen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Index does not support mutable operations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: Index does not support mutable operations"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This immutability is a good thing: it makes it safer to share indices between multiple ``Series`` or ``DataFrame``s without the potential for problems arising from inadvertent index modification."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In addition to being array-like, a Index also behaves like a fixed-size set, including following many of the conventions used by Python's built-in ``set`` data structure, so that unions, intersections, differences, and other combinations can be computed in a familiar way. Let's play around with this to see it in action."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "ind_odd = pd.Index([1, 3, 5, 7, 9])\nind_prime = pd.Index([2, 3, 5, 7, 11])",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the code cell below, try out the intersection (`ind_odd & ind_prime`), union (`ind_odd | ind_prime`), and the symmetric difference (`ind_odd ^ ind_prime`) of `ind_odd` and `ind_prime`."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These operations may also be accessed via object methods, for example ``ind_odd.intersection(ind_prime)``. Below is a table listing some useful `Index` methods and properties."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "| **Method**     | **Description**                                                                           |\n|:---------------|:------------------------------------------------------------------------------------------|\n| [`append`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html)       | Concatenate with additional `Index` objects, producing a new `Index`                      |\n| [`diff`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html)         | Compute set difference as an Index                                                        |\n| [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)         | Compute new `Index` by deleting passed values                                             |\n| [`insert`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.insert.html)       | Compute new `Index` by inserting element at index `i`                                     |\n| [`is_monotonic`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.is_monotonic.html) | Returns `True` if each element is greater than or equal to the previous element           |\n| [`is_unique`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.is_unique.html)    | Returns `True` if the Index has no duplicate values                                       |\n| [`isin`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html)         | Compute boolean array indicating whether each value is contained in the passed collection |\n| [`unique`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html)       | Compute the array of unique values in order of appearance                                         |"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Data Selection in Series\n\nAs a refresher, a ``Series`` object acts in many ways like both a one-dimensional `ndarray` and a standard Python dictionary.\n\nLike a dictionary, the ``Series`` object provides a mapping from a collection of arbitrary keys to a collection of arbitrary values. Back to an old example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2 = pd.Series([-0.5, 0.75, 1.0, -2], index=['a', 'b', 'c', 'd'])\nseries_example2",
      "execution_count": 72,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\nc    1.00\nd   -2.00\ndtype: float64"
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2['b']",
      "execution_count": 74,
      "outputs": [
        {
          "data": {
            "text/plain": "0.75"
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also examine the keys/indices and values using dictionary-like Python tools:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "'a' in series_example2",
      "execution_count": 75,
      "outputs": [
        {
          "data": {
            "text/plain": "True"
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2.keys()",
      "execution_count": 76,
      "outputs": [
        {
          "data": {
            "text/plain": "Index(['a', 'b', 'c', 'd'], dtype='object')"
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "list(series_example2.items())",
      "execution_count": 77,
      "outputs": [
        {
          "data": {
            "text/plain": "[('a', -0.5), ('b', 0.75), ('c', 1.0), ('d', -2.0)]"
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As with dictionaries, you can extend a dictionary by assigning to a new key, you can extend a ``Series`` by assigning to a new index value:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2['e'] = 1.25\nseries_example2",
      "execution_count": 78,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\nc    1.00\nd   -2.00\ne    1.25\ndtype: float64"
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Series as one-dimensional array\n\nBecause ``Series`` also provide array-style functionality, you can use the NumPy techniques we looked at a previous section like slices, masking, and fancy indexing:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Slicing using the explicit index\nseries_example2['a':'c']",
      "execution_count": 79,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\nc    1.00\ndtype: float64"
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Slicing using the implicit integer index\nseries_example2[0:2]",
      "execution_count": 80,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\ndtype: float64"
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Masking\nseries_example2[(series_example2 > -1) & (series_example2 < 0.8)]",
      "execution_count": 82,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\ndtype: float64"
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Fancy indexing\nseries_example2[['a', 'e']]",
      "execution_count": 84,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\ne    1.25\ndtype: float64"
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One note to avoid confusion. When slicing with an explicit index (i.e., ``series_example2['a':'c']``), the final index is **included** in the slice; when slicing with an implicit index (i.e., ``series_example2[0:2]``), the final index is **excluded** from the slice."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Indexers: `loc` and `iloc`\n\nA great thing about pandas is that you can use a lot different things for your explicit indices. A potentially confusing thing about pandas is that you can use a lot different things for your explicit indices, including integers. To avoid confusion between integer indices that you might supply and those implicit integer indices that pandas generates, pandas provides special *indexer* attributes that explicitly expose certain indexing schemes.\n\n(A technical note: These are not functional methods; they are attributes that expose a particular slicing interface to the data in the ``Series``.)\n\nThe ``loc`` attribute allows indexing and slicing that always references the explicit index:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2.loc['a']",
      "execution_count": 86,
      "outputs": [
        {
          "data": {
            "text/plain": "-0.5"
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2.loc['a':'c']",
      "execution_count": 88,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\nc    1.00\ndtype: float64"
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The ``iloc`` attribute enables indexing and slicing using the implicit, Python-style index:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2.iloc[0]",
      "execution_count": 90,
      "outputs": [
        {
          "data": {
            "text/plain": "-0.5"
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series_example2.iloc[0:2]",
      "execution_count": 91,
      "outputs": [
        {
          "data": {
            "text/plain": "a   -0.50\nb    0.75\ndtype: float64"
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A guiding principle of the Python language is the idea that \"explicit is better than implicit.\" Professional code will generally use explicit indexing with ``loc`` and ``iloc`` and you should as well in order to make your code clean and readable."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Data selection in DataFrames\n\n``DataFrame``s also exhibit dual behavior, acting both like a two-dimensional `ndarray` and like a dictionary of ``Series``  sharing the same index."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### DataFrame as dictionary of Series\n\nLet's return to our earlier example of countries' areas and populations in order to examine `DataFrame`s as a dictionary of `Series`."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "area = pd.Series({'Albania': 28748,\n                  'France': 643801,\n                  'Germany': 357386,\n                  'Japan': 377972,\n                  'Russia': 17125200})\npopulation = pd.Series ({'Albania': 2937590,\n                         'France': 65429495,\n                         'Germany': 82408706,\n                         'Russia': 143910127,\n                         'Japan': 126922333})\ncountries = pd.DataFrame({'Area': area, 'Population': population})\ncountries",
      "execution_count": 92,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>28748</td>\n      <td>2937590</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>377972</td>\n      <td>126922333</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>17125200</td>\n      <td>143910127</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "             Area  Population\nAlbania     28748     2937590\nFrance     643801    65429495\nGermany    357386    82408706\nJapan      377972   126922333\nRussia   17125200   143910127"
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can access the individual ``Series`` that make up the columns of a ``DataFrame`` via dictionary-style indexing of the column name:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries['Area']",
      "execution_count": 94,
      "outputs": [
        {
          "data": {
            "text/plain": "Albania       28748\nFrance       643801\nGermany      357386\nJapan        377972\nRussia     17125200\nName: Area, dtype: int64"
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "An you can use dictionary-style syntax can also be used to modify `DataFrame`s, such as by adding a new column:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries['Population Density'] = countries['Population'] / countries['Area']\ncountries",
      "execution_count": 95,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>28748</td>\n      <td>2937590</td>\n      <td>102.184152</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>101.629999</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>335.798242</td>\n    </tr>\n    <tr>\n      <th>Russia</th>\n      <td>17125200</td>\n      <td>143910127</td>\n      <td>8.403413</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "             Area  Population  Population Density\nAlbania     28748     2937590          102.184152\nFrance     643801    65429495          101.629999\nGermany    357386    82408706          230.587393\nJapan      377972   126922333          335.798242\nRussia   17125200   143910127            8.403413"
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### DataFrame as two-dimensional array\n\nYou can also think of ``DataFrame``s as two-dimensional arrays. You can examine the raw data in the `DataFrame`/data array using the ``values`` attribute:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries.values",
      "execution_count": 96,
      "outputs": [
        {
          "data": {
            "text/plain": "array([[  2.87480000e+04,   2.93759000e+06,   1.02184152e+02],\n       [  6.43801000e+05,   6.54294950e+07,   1.01629999e+02],\n       [  3.57386000e+05,   8.24087060e+07,   2.30587393e+02],\n       [  3.77972000e+05,   1.26922333e+08,   3.35798242e+02],\n       [  1.71252000e+07,   1.43910127e+08,   8.40341292e+00]])"
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Viewed thsi way it makes sense that we can transpose the rows and columns of a `DataFrame` the same way we would an array:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries.T",
      "execution_count": 97,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Albania</th>\n      <th>France</th>\n      <th>Germany</th>\n      <th>Japan</th>\n      <th>Russia</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Area</th>\n      <td>2.874800e+04</td>\n      <td>6.438010e+05</td>\n      <td>3.573860e+05</td>\n      <td>3.779720e+05</td>\n      <td>1.712520e+07</td>\n    </tr>\n    <tr>\n      <th>Population</th>\n      <td>2.937590e+06</td>\n      <td>6.542950e+07</td>\n      <td>8.240871e+07</td>\n      <td>1.269223e+08</td>\n      <td>1.439101e+08</td>\n    </tr>\n    <tr>\n      <th>Population Density</th>\n      <td>1.021842e+02</td>\n      <td>1.016300e+02</td>\n      <td>2.305874e+02</td>\n      <td>3.357982e+02</td>\n      <td>8.403413e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                         Albania        France       Germany         Japan  \\\nArea                2.874800e+04  6.438010e+05  3.573860e+05  3.779720e+05   \nPopulation          2.937590e+06  6.542950e+07  8.240871e+07  1.269223e+08   \nPopulation Density  1.021842e+02  1.016300e+02  2.305874e+02  3.357982e+02   \n\n                          Russia  \nArea                1.712520e+07  \nPopulation          1.439101e+08  \nPopulation Density  8.403413e+00  "
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "`DataFrame`s also uses the ``loc`` and ``iloc`` indexers. With ``iloc``, you can index the underlying array as if it were an `ndarray` but with the ``DataFrame`` index and column labels maintained in the result:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries.iloc[:3, :2]",
      "execution_count": 98,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>28748</td>\n      <td>2937590</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           Area  Population\nAlbania   28748     2937590\nFrance   643801    65429495\nGermany  357386    82408706"
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "``loc`` also permits array-like slicing but using the explicit index and column names:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries.loc[:'Germany', :'Population']",
      "execution_count": 100,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Albania</th>\n      <td>28748</td>\n      <td>2937590</td>\n    </tr>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           Area  Population\nAlbania   28748     2937590\nFrance   643801    65429495\nGermany  357386    82408706"
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "You can also use array-like techniques such as masking and fancing indexing with `loc`."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Can you think of how to combine masking and fancy indexing in one line?\n# Your masking could be somthing like countries['Population Density'] > 200\n# Your fancy indexing could be something like ['Population', 'Population Density']\n# Be sure to put the the masking and fancy indexing inside the square brackets: countries.loc[]\n",
      "execution_count": 101,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Indexing conventions\n\nIn practice in the world of data science (and pandas more generally), *indexing* refers to columns while *slicing* refers to rows:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries['France':'Japan']",
      "execution_count": 102,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>101.629999</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>335.798242</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           Area  Population  Population Density\nFrance   643801    65429495          101.629999\nGermany  357386    82408706          230.587393\nJapan    377972   126922333          335.798242"
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Such slices can also refer to rows by number rather than by index:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries[1:3]",
      "execution_count": 103,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>France</th>\n      <td>643801</td>\n      <td>65429495</td>\n      <td>101.629999</td>\n    </tr>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           Area  Population  Population Density\nFrance   643801    65429495          101.629999\nGermany  357386    82408706          230.587393"
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similarly, direct masking operations are also interpreted row-wise rather than column-wise:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "countries[countries['Population Density'] > 200]",
      "execution_count": 104,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Population</th>\n      <th>Population Density</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Germany</th>\n      <td>357386</td>\n      <td>82408706</td>\n      <td>230.587393</td>\n    </tr>\n    <tr>\n      <th>Japan</th>\n      <td>377972</td>\n      <td>126922333</td>\n      <td>335.798242</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "           Area  Population  Population Density\nGermany  357386    82408706          230.587393\nJapan    377972   126922333          335.798242"
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These two conventions are syntactically similar to those on a NumPy array, and while these may not precisely fit the mold of the Pandas conventions, they are nevertheless quite useful in practice."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Operating on Data in Pandas\n\nAs you begin to work in data science, operating on data is imperative. It is the very heart of data science. Another aspect of pandas that makes it a compelling tool for many data scientists is pandas' capability to perform efficient element-wise operations on data. pandas builds on ufuncs from NumPy to supply theses capabilities and then extends them to provide additional power for data manipulation:\n - For unary operations (such as negation and trigonometric functions), ufuncs in pandas **preserve index and column labels** in the output.\n - For binary operations (such as addition and multiplication), pandas automatically **aligns indices** when passing objects to  ufuncs.\n\nThese critical features of ufuncs in pandas mean that data retains its context when operated on and, more importantly still, drastically helps reduce errors when you combine data from multiple sources."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Index Preservation\n\npandas is explicitly designed to work with NumPy. As a results, all NumPy ufuncs will work on Pandas ``Series`` and ``DataFrame`` objects.\n\nWe can see this more clearly if we create a simple ``Series`` and ``DataFrame`` of random numbers on which to operate. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "rng = np.random.RandomState(42)\nser_example = pd.Series(rng.randint(0, 10, 4))\nser_example",
      "execution_count": 8,
      "outputs": [
        {
          "data": {
            "text/plain": "0    6\n1    3\n2    7\n3    4\ndtype: int32"
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Did you notice the NumPy function we used with the variable `rng`? By specifying a seed for the random-number generator, you get the same result each time. This can be useful trick when you need to produce psuedo-random output that also needs to be replicatable by others. (Go ahead and re-run the code cell above a couple of times to convince yourself that it produces the same output each time.)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df_example = pd.DataFrame(rng.randint(0, 10, (3, 4)),\n                  columns=['A', 'B', 'C', 'D'])\ndf_example",
      "execution_count": 9,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>9</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>4</td>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   A  B  C  D\n0  6  9  2  6\n1  7  4  3  7\n2  7  2  5  4"
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's apply a ufunc to our example `Series`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.exp(ser_example)",
      "execution_count": 11,
      "outputs": [
        {
          "data": {
            "text/plain": "0     403.428793\n1      20.085537\n2    1096.633158\n3      54.598150\ndtype: float64"
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The same thing happens with a slightly more complex operation on our example `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.cos(df_example * np.pi / 4)",
      "execution_count": 12,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.836970e-16</td>\n      <td>7.071068e-01</td>\n      <td>6.123234e-17</td>\n      <td>-1.836970e-16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.071068e-01</td>\n      <td>-1.000000e+00</td>\n      <td>-7.071068e-01</td>\n      <td>7.071068e-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.071068e-01</td>\n      <td>6.123234e-17</td>\n      <td>-7.071068e-01</td>\n      <td>-1.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "              A             B             C             D\n0 -1.836970e-16  7.071068e-01  6.123234e-17 -1.836970e-16\n1  7.071068e-01 -1.000000e+00 -7.071068e-01  7.071068e-01\n2  7.071068e-01  6.123234e-17 -7.071068e-01 -1.000000e+00"
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that you can use all of the ufuncs we discussed in a previous section the same way."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Index alignment\n\nAs mentioned above, when you perform a binary operation on two ``Series`` or ``DataFrame`` objects, pandas will align indices in the process of performing the operation. This is essential when working with incomplete data (and data is usually incomplete), but it is helpful to see this in action to better understand it."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Index alignment with Series\n\nFor our first example, suppose we are combining two different data sources and find only the top five countries by *area* and the top five countries by *population*:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "area = pd.Series({'Russia': 17075400, 'Canada':  9984670,\n                  'USA': 9826675, 'China': 9598094, \n                  'Brazil': 8514877}, name='area')\npopulation = pd.Series({'China': 1409517397, 'India': 1339180127,\n                        'USA': 324459463, 'Indonesia': 322179605, \n                        'Brazil': 207652865}, name='population')",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now divide these to compute the population density\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Your resulting array contains the **union** of indices of the two input arrays: seven countries in total. All of the countries in the array without an entry (because they lacked either area data or population data) are marked with the now familiar ``NaN``, or \"Not a Number,\" designation.\n\nIndex matching works the same way built-in Python arithmetic expressions and missing values are filled in with `NaN`s. You can see this clearly by adding two `Series` that are slightly misaligned in their indices:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series1 = pd.Series([2, 4, 6], index=[0, 1, 2])\nseries2 = pd.Series([3, 5, 7], index=[1, 2, 3])\nseries1 + series2",
      "execution_count": 18,
      "outputs": [
        {
          "data": {
            "text/plain": "0     NaN\n1     7.0\n2    11.0\n3     NaN\ndtype: float64"
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "`NaN` values are not always convenient to work with; `NaN` combined with any other values results in `NaN`, which can be a pain, particulalry if you are combining multiple data sources with missing values. To help with this, pandas allows you to specify a default value to use for missing values in the operation. For example, calling `series1.add(series2)` is equivalent to calling `series1 + series2`, but you can supply the fill value:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "series1.add(series2, fill_value=0)",
      "execution_count": 19,
      "outputs": [
        {
          "data": {
            "text/plain": "0     2.0\n1     7.0\n2    11.0\n3     7.0\ndtype: float64"
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Much better!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Index alignment with DataFrames\n\nThe same kind of alignment takes place in both dimension (columns and indices) when you perform operations on ``DataFrame``s."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df1 = pd.DataFrame(rng.randint(0, 20, (2, 2)),\n                   columns=list('AB'))\ndf1",
      "execution_count": 20,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   A   B\n0  1  11\n1  5   1"
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df2 = pd.DataFrame(rng.randint(0, 10, (3, 3)),\n                   columns=list('BAC'))\ndf2",
      "execution_count": 22,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>B</th>\n      <th>A</th>\n      <th>C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>8</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   B  A  C\n0  3  8  2\n1  4  2  6\n2  4  8  6"
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Add df1 and df2. Is the output what you expected?",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Even though we passed the columns in a different order in `df2` than in `df1`, the indices were aligned correctly sorted in the resulting union of columns.\n\nYou can also use fill values for missing values with `Data Frame`s. In this example, let's fill the missing values with the mean of all values in `df1` (computed by first stacking the rows of `df1`):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "fill = df1.stack().mean()\ndf1.add(df2, fill_value=fill)",
      "execution_count": 24,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9.0</td>\n      <td>14.0</td>\n      <td>6.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>10.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12.5</td>\n      <td>8.5</td>\n      <td>10.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "      A     B     C\n0   9.0  14.0   6.5\n1   7.0   5.0  10.5\n2  12.5   8.5  10.5"
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This table lists Python operators and their equivalent pandas object methods:\n\n| Python Operator | Pandas Method(s)                      |\n|-----------------|---------------------------------------|\n| ``+``           | ``add()``                             |\n| ``-``           | ``sub()``, ``subtract()``             |\n| ``*``           | ``mul()``, ``multiply()``             |\n| ``/``           | ``truediv()``, ``div()``, ``divide()``|\n| ``//``          | ``floordiv()``                        |\n| ``%``           | ``mod()``                             |\n| ``**``          | ``pow()``                             |\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Operations between DataFrames and Series\n\nIndex and column alignment gets maintained in operations between a `DataFrame` and a `Series` as well. To see this, consider a common operation in data science, wherein we find the difference of a `DataFrame` and one of its rows. Because pandas inherits ufuncs from NumPy, pandas will compute the difference row-wise by default:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df3 = pd.DataFrame(rng.randint(10, size=(3, 4)), columns=list('WXYZ'))\ndf3",
      "execution_count": 25,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>W</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>8</td>\n      <td>9</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   W  X  Y  Z\n0  1  3  8  1\n1  9  8  9  4\n2  1  3  6  7"
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df3 - df3.iloc[0]",
      "execution_count": 26,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>W</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>-2</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   W  X  Y  Z\n0  0  0  0  0\n1  8  5  1  3\n2  0  0 -2  6"
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "But what if you need to operate column-wise? You can do this by using object methodsand specifying the ``axis`` keyword."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df3.subtract(df3['X'], axis=0)",
      "execution_count": 28,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>W</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>-2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   W  X  Y  Z\n0 -2  0  5 -2\n1  1  0  1 -4\n2 -2  0  3  4"
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And when you do operations between `DataFrame`s and `Series` operations, you still get automatic index alignment:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "halfrow = df3.iloc[0, ::2]\nhalfrow",
      "execution_count": 29,
      "outputs": [
        {
          "data": {
            "text/plain": "W    1\nY    8\nName: 0, dtype: int32"
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that the output from that operation was transposed. That was so that we can subtract it from the `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df3 - halfrow",
      "execution_count": 30,
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>W</th>\n      <th>X</th>\n      <th>Y</th>\n      <th>Z</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>-2.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "     W   X    Y   Z\n0  0.0 NaN  0.0 NaN\n1  8.0 NaN  1.0 NaN\n2  0.0 NaN -2.0 NaN"
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember, pandas preserves and aligns indices and columns so preserve data context. This will be of huge help to you in our next section when we look at data cleaning and preparation."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 5: Manipulating and Cleaning Data\n\nThis section marks a subtle change. Up until now, we have been introducing ideas and techniques in order to prepare you with a toolbox of techniques to deal with real-world situations. We are now going to start using some of those tools while also giving you some ideas about how and when to use them in your own work with data.\n\nReal-world data is messy. You will likely need to combine several data sources to get the data you actually want. The data from those sources will be incomplete. And it will likely not be formatted in exactly the way you want in order to perform your analysis. It's for these reasons that most data scientists will tell you that about 80 percent of any project is spent just getting the data into a form ready for analysis.\n\n## Exploring `DataFrame` information\n\n> **Learning goal:** By the end of this subsection, you should be comfortable finding general information about the data stored in pandas DataFrames.\n\nOnce you have loaded your data into pandas, it will more likely than not be in a `DataFrame`. However, if the data set in your `DataFrame` has 60,000 rows and 400 columns, how do you even begin to get a sense of what you're working with? Fortunately, pandas provides some conventient tools to quickly look at overall information about a `DataFrame` in addition to the first few and last few rows.\n\nIn order to explore this functionality, we will import the Python scikit-learn library and use an iconic dataset that every data scientist has seen hundreds of times: British biologist Ronald Fisher's *Iris* data set used in his 1936 paper \"The use of multiple measurements in taxonomic problems\":"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris_df = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `DataFrame.info`\nLet's take a look at this dataset to see what we have:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "iris_df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "From this, we know that the *Iris* dataset has 150 entries in four columns. All of the data is stored as 64-bit floating-point numbers."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `DataFrame.head`\nNext, let's see what the first few rows of our `DataFrame` look like:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "iris_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**\n\nBy default, `DataFrame.head` returns the first five rows of a `DataFrame`. In the code cell below, can you figure out how to get it to show more?"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Hint: Consult the documentation by using iris_df.head?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `DataFrame.tail`\nThe flipside of `DataFrame.head` is `DataFrame.tail`, which returns the last five rows of a `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "iris_df.tail()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In practice, it is useful to be able to easily examine the first few rows or the last few rows of a `DataFrame`, particularly when you are looking for outliers in ordered datasets.\n\n> **Takeaway:** Even just by looking at the metadata about the information in a DataFrame or the first and last few values in one, you can get an immediate idea about the size, shape, and content of the data you are dealing with."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Dealing with missing data\n\n> **Learning goal:** By the end of this subsection, you should know how to replace or remove null values from DataFrames.\n\nMost of the time the datasets you want to use (of have to use) have missing values in them. How missing data is handled carries with it subtle tradeoffs that can affect your final analysis and real-world outcomes.\n\nPandas handles missing values in two ways. The first you've seen before in previous sections: `NaN`, or Not a Number. This is a actually a special value that is part of the IEEE floating-point specification and it is only used to indicate missing floating-point values.\n\nFor missing values apart from floats, pandas uses the Python `None` object. While it might seem confusing that you will encounter two different kinds of values that say essentially the same thing, there are sound programmatic reasons for this design choice and, in practice, going this route enables pandas to deliver a good compromise for the vast majority of cases. Notwithstanding this, both `None` and `NaN` carry restrictions that you need to be mindful of with regards to how they can be used."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `None`: non-float missing data\nBecause `None` comes from Python, it cannot be used in NumPy and pandas arrays that are not of data type `'object'`. Remember, NumPy arrays (and the data structures in pandas) can contain only one type of data. This is what gives them their tremendous power for large-scale data and computational work, but it also limits their flexibility. Such arrays have to upcast to the “lowest common denominator,” the data type that will encompass everything in the array. When `None` is in the array, it means you are working with Python objects.\n\nTo see this in action, consider the following example array (note the `dtype` for it):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nexample1 = np.array([2, None, 6, 8])\nexample1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The reality of upcast data types carries two side effects with it. First, operations will be carried out at the level of interpreted Python code rather than compiled NumPy code. Essentially, this means that any operations involving `Series` or `DataFrames` with `None` in them will be slower. While you would probably not notice this performance hit, for large datasets it might become an issue.\n\nThe second side effect stems from the first. Because `None` essentially drags `Series` or `DataFrame`s back into the world of vanilla Python, using NumPy/pandas aggregations like `sum()` or `min()` on arrays that contain a ``None`` value will generally produce an error:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example1.sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Key takeaway**: Addition (and other operations) between integers and `None` values is undefined, which can limit what you can do with datasets that contain them."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `NaN`: missing float values\n\nIn contrast to `None`, NumPy (and therefore pandas) supports `NaN` for its fast, vectorized operations and ufuncs. The bad news is that any arithmetic performed on `NaN` always results in `NaN`. For example:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.nan + 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "np.nan * 0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The good news: aggregations run on arrays with `NaN` in them don't pop errors. The bad news: the results are not uniformly useful:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example2 = np.array([2, np.nan, 6, 8]) \nexample2.sum(), example2.min(), example2.max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What happens if you add np.nan and None together?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Remember: `NaN` is just for missing floating-point values; there is no `NaN` equivalent for integers, strings, or Booleans."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### `NaN` and `None`: null values in pandas\n\nEven though `NaN` and `None` can behave somewhat differently, pandas is nevertheless built to handle them interchangeably. To see what we mean, consider a `Series` of integers:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "int_series = pd.Series([1, 2, 3], dtype=int)\nint_series",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now set an element of int_series equal to None.\n# How does that element show up in the Series?\n# What is the dtype of the Series?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In the process of upcasting data types to establish data homogeneity in `Seires` and `DataFrame`s, pandas will willingly switch missing values between `None` and `NaN`. Because of this design feature, it can be helpful to think of `None` and `NaN` as two different flavors of \"null\" in pandas. Indeed, some of the core methods you will use to deal with missing values in pandas reflect this idea in their names:\n\n- `isnull()`: Generates a Boolean mask indicating missing values\n- `notnull()`: Opposite of `isnull()`\n- `dropna()`: Returns a filtered version of the data\n- `fillna()`: Returns a copy of the data with missing values filled or imputed\n\nThese are important methods to master and get comfortable with, so let's go over them each in some depth."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Detecting null values\nBoth `isnull()` and `notnull()` are your primary methods for detecting null data. Both return Boolean masks over your data."
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "example3 = pd.Series([0, np.nan, '', None])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example3.isnull()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Look closely at the output. Does any of it surprise you? While `0` is an arithmetic null, it's nevertheless a perfectly good integer and pandas treats it as such. `''` is a little more subtle. While we used it in the first section to represent an empty string value, it is nevertheless a string object and not a representation of null as far as pandas is concerned.\n\nNow, let's turn this around and use these methods in a manner more like you will use them in practice. You can use Boolean masks  directly as a ``Series`` or ``DataFrame`` index, which can be useful when trying to work with isolated missing (or present) values."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Try running example3[example3.notnull()].\n# Before you do so, what do you expect to see?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Key takeaway**: Both the `isnull()` and `notnull()` methods produce similar results when you use them in `DataFrame`s: they show the results and the index of those results, which will help you enormously as you wrestle with your data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dropping null values\n\nBeyond identifying missing values, pandas provides a convenient means to remove null values from `Series` and `DataFrame`s. (Particularly on large data sets, it is often more advisable to simply remove missing [NA] values from your analysis than deal with them in other ways.) To see this in action, let's return to `example3`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example3 = example3.dropna()\nexample3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that this should look like your output from `example3[example3.notnull()]`. The difference here is that, rather than just indexing on the masked values, `dropna` has removed those missing values from the `Series` `example3`.\n\nBecause `DataFrame`s have two dimensions, they afford more options for dropping data."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4 = pd.DataFrame([[1,      np.nan, 7], \n                         [2,      5,      8], \n                         [np.nan, 6,      9]])\nexample4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "(Did you notice that pandas upcast two of the columns to floats to accommodate the `NaN`s?)\n\nYou cannot drop a single value from a `DataFrame`, so you have to drop full rows or columns. Depending on what you are doing, you might want to do one or the other, and so pandas gives you options for both. Because in data science, columns generally represent variables and rows represent observations, you are more likely to drop rows of data; the default setting for `dropna()` is to drop all rows that contain any null values:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4.dropna()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If necessary, you can drop NA values from columns. Use `axis=1` to do so:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4.dropna(axis='columns')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that this can drop a lot of data that you might want to keep, particularly in smaller datasets. What if you just want to drop rows or columns that contain several or even just all null values? You specify those setting in `dropna` with the `how` and `thresh` parameters.\n\nBy default, `how='any'` (if you would like to check for yourself or see what other parameters the method has, run `example4.dropna?` in a code cell). You could alternatively specify `how='all'` so as to drop only rows or columns that contain all null values. Let's expand our example `DataFrame` to see this in action."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4[3] = np.nan\nexample4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# How might you go about dropping just column 3?\n# Hint: remember that you will need to supply both the axis parameter and the how parameter.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The `thresh` parameter gives you finer-grained control: you set the number of *non-null* values that a row or column needs to have in order to be kept:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4.dropna(axis='rows', thresh=3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Here, the first and last row have been dropped, because they contain only two non-null values."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Filling null values\n\nDepending on your dataset, it can sometimes make more sense to fill null values with valid ones rather than drop them. You could use `isnull` to do this in place, but that can be laborious, particularly if you have a lot of values to fill. Because this is such a common task in data science, pandas provides `fillna`, which returns a copy of the `Series` or `DataFrame` with the missing values replaced with one of your choosing. Let's create another example `Series` to see how this works in practice."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example5 = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))\nexample5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can fill all of the null entries with a single value, such as `0`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example5.fillna(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What happens if you try to fill null values with a string, like ''?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can **forward-fill** null values, which is to use the last valid value to fill a null:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example5.fillna(method='ffill')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also **back-fill** to propagate the next valid value backward to fill a null:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example5.fillna(method='bfill')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "As you might guess, this works the same with `DataFrame`s, but you can also specify an `axis` along which to fill null values:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4.fillna(method='ffill', axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that when a previous value is not available for forward-filling, the null value remains."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What output does example4.fillna(method='bfill', axis=1) produce?\n# What about example4.fillna(method='ffill') or example4.fillna(method='bfill')?\n# Can you think of a longer code snippet to write that can fill all of the null values in example4?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can be creative about how you use `fillna`. For example, let's look at `example4` again, but this time let's fill the missing values with the average of all of the values in the `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example4.fillna(example4.mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that column 3 is still valueless: the default direction is to fill values row-wise.\n\n> **Takeaway:** There are multiple ways to deal with missing values in your datasets. The specific strategy you use (removing them, replacing them, or even how you replace them) should be dictated by the particulars of that data. You will develop a better sense of how to deal with missing values the more you handle and interact with datasets."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Removing duplicate data\n\n> **Learning goal:** By the end of this subsection, you should be comfortable identifying and removing duplicate values from DataFrames.\n\nIn addition to missing data, you will often encounter duplicated data in real-world datasets. Fortunately, pandas provides an easy means of detecting and removing duplicate entries."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Identifying duplicates: `duplicated`\n\nYou can easily spot duplicate values using the `duplicated` method in pandas, which returns a Boolean mask indicating whether an entry in a `DataFrame` is a duplicate of an ealier one. Let's create another example `DataFrame` to see this in action."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example6 = pd.DataFrame({'letters': ['A','B'] * 2 + ['B'],\n                         'numbers': [1, 2, 1, 3, 3]})\nexample6",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example6.duplicated()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Dropping duplicates: `drop_duplicates`\n`drop_duplicates` simply returns a copy of the data for which all of the `duplicated` values are `False`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example6.drop_duplicates()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Both `duplicated` and `drop_duplicates` default to consider all columnsm but you can specify that they examine only a subset of columns in your `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "example6.drop_duplicates(['letters'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** Removing duplicate data is an essential part of almost every data-science project. Duplicate data can change the results of your analyses and give you spurious results!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Combining datasets: merge and join\n\n> **Learning goal:** By the end of this subsection, you should have a general knowledge of the various ways to combine `DataFrame`s.\n\nYour most interesting analyses will often come from data melded together from more than one source. Because of this, pandas provides several methods of merging and joining datasets to make this necessary job easier:\n - **`pandas.merge`** connects rows in `DataFrame`s based on one or more keys.\n - **`pandas.concat`** concatenates or “stacks” together objects along an axis.\n - The **`combine_first`** instance method enables you to splice together overlapping data to fill in missing values in one object with values from another.\n\nLet's examine merging data first, because it will be the most familiar to course attendees who are already familiar with SQL or other relational databases."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Categories of joins\n\n`merge` carries out several types of joins: *one-to-one*, *many-to-one*, and *many-to-many*. You use the same basic function call to implement all of them and we will examine all three (because you will need all three as some point in your data delving depending on the data). We will start with one-to-one joins because they are generally the simplest example."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### One-to-one joins\n\nConsider combining two `DataFrame`s that contain different information on the same employees in a company:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df1 = pd.DataFrame({'employee': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'group': ['Accounting', 'Marketing', 'Marketing', 'HR']})\ndf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df2 = pd.DataFrame({'employee': ['Mary', 'Stu', 'Gary', 'Sue'],\n                    'hire_date': [2008, 2012, 2017, 2018]})\ndf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Combine this information into a single `DataFrame` using the `merge` function:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df3 = pd.merge(df1, df2)\ndf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Pandas joined on the `employee` column because it was the only column common to both `df1` and `df2`. (Note also that the original indices of `df1` and `df2` were discarded by `merge`; this is generally the case with merges unless you conduct them by index, which we will dicuss later on.)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Many-to-one joins\n\nA many-to-one join is like a one-to-one join except that one of the two key columns contains duplicate entries. The `DataFrame` resulting from such a join will preserve those duplicate entries as appropriate:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df4 = pd.DataFrame({'group': ['Accounting', 'Marketing', 'HR'],\n                    'supervisor': ['Carlos', 'Giada', 'Stephanie']})\ndf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df3, df4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The resulting `DataFrame` has an additional column for `supervisor`; that column has an extra occurence of 'Giada' that did not occur in `df4` because more than one employee in the merged `DataFrame` works in the 'Marketing' group.\n\nNote that we didn’t specify which column to join on. When you don't specify that information, `merge` uses the overlapping column names as the keys. However, that can be ambiguous; several columns might meet that condition. For that reason, it is a good practice to explicitly specify on which key to join. You can do this with the `on` parameter:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df3, df4, on='group')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Many-to-many joins\nWhat happens if the key columns in both of the `DataFrame`s you are joining contain duplicates? That gives you a many-to-many join:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n                              'Marketing', 'Marketing', 'HR', 'HR'],\n                    'core_skills': ['math', 'spreadsheets', 'writing', 'communication',\n                               'spreadsheets', 'organization']})\ndf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, on='group')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, in order to avoid ambiguity as to which column to join on, it is a good idea to explicitly tell `merge` which one to use with the `on` parameter."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `left_on` and `right_on` keywords\nWhat if you need to merge two datasets with no shared column names? For example, what if you are using a dataset in which the employee name is labeled as 'name' rather than 'employee'? In such cases, you will need to use the `left_on` and `right_on` keywords in order to specify the column names on which to join:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df6 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'salary': [70000, 80000, 120000, 90000]})\ndf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df6, left_on=\"employee\", right_on=\"name\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Using the documentation, can you figure out how to use .drop() to get rid of the 'name' column?\n# Hint: You will need to supply two parameters to .drop()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `left_index` and `right_index` keywords\n\nSometimes it can be more advantageous to merge on an index rather than on a column. The `left_index` and `right_index` keywords make it possible to join by index. Let's revisit some of our earlier example `DataFrame`s to see what this looks like in action."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df1a = df1.set_index('employee')\ndf1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df2a = df2.set_index('employee')\ndf2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To merge on the index, specify the `left_index` and `right_index` parameters in `merge`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df1a, df2a, left_index=True, right_index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What happens if you specify only left_index or right_index?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also use the `join` method for `DataFrame`s, which produces the same effect but merges on indices by default:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df1a.join(df2a)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also mix and match `left_index`/`right_index` with `right_on`/`left_on`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df1a, df6, left_index=True, right_on='name')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Set arithmetic for joins\n\nLet's return to many-to-many joins for a moment. A consideration that is unique to them is the *arithmetic* of the join, specifically the set arithmetic we use for the join. To illustrate what we mean by this, let's restructure an old example `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df5 = pd.DataFrame({'group': ['Engineering', 'Marketing', 'Sales'],\n                    'core_skills': ['math', 'writing', 'communication']})\ndf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, on='group')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that after we have restructured `df5` and then re-run the merge with `df1`, we have only two entries in the result. This is because we merged on `group` and 'Marketing' was the only entry that appeared in the `group` column of both `DataFrame`s.\n\nIn effect, what we have gotten is the *intersection* of both `DataFrame`s. This is know as the inner join in the database world and it is the default setting for `merge` although we can certainly specify it:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, on='group', how='inner')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The complement of the inner join is the outer join, which returns the *union* of the two `DataFrame`s."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# The keyword for perfoming an outer join is how='outer'. How would you perform it?\n# What do you expect the output of an outer join of df1 and df5 to be?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice in your resulting `DataFrame` that not every row in `df1` and `df5` had a value that corresponds to the union of the key values (the 'group' column). Pandas fills in these missing values with `NaN`s."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Inner and outer joins are not your only options. A *left join* returns all of the rows in the first (left-side) `DataFrame` supplied to `merge` along with rows from the other `DataFrame` that match up with the left-side key values (and `NaNs` rows with respective values):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df1, df5, how='left')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now run the right merge between df1 and df5.\n# What do you expect to see?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `suffixes` keyword: dealing with conflicting column names\nBecause you can join datasets, you will eventually join two with conflicting column names. Let's look at another example to see what we mean:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df7 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'rank': [1, 2, 3, 4]})\ndf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df8 = pd.DataFrame({'name': ['Gary', 'Stu', 'Mary', 'Sue'],\n                    'rank': [3, 1, 4, 2]})\ndf8",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df7, df8, on='name')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Each column name in a `DataFrame` must be unique, so in cases where two joined `DataFrame`s share column names (aside from the column serving as the key), the `merge` function automatically appends the suffix `_x` or `_y` to the conflicting column names in order to make them unique. In cases where it is best to control your column names, you can specify a custom suffix for `merge` to append through the `suffixes` keyword:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.merge(df7, df8, on='name', suffixes=['_left', '_right'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that these suffixes work if there are multiple conflicting columns."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Concatenation in NumPy\nConcatenation in pandas is built off of the concatenation functionality for NumPy arrays. Here is what NumPy concatenation looks like:\n - For one-dimensional arrays:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "x = [1, 2, 3]\ny = [4, 5, 6]\nz = [7, 8, 9]\nnp.concatenate([x, y, z])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": " - For two-dimensional arrays:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "x = [[1, 2],\n     [3, 4]]\nnp.concatenate([x, x], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the `axis=1` parameter makes the concatenation occur along columns rather than rows. Concatenation in pandas looks similar to this."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Concatenation in pandas\n\nPandas has a function, `pd.concat()` that can be used for a simple concatenation of `Series` or `DataFrame` objects in similar manner to `np.concatenate()` with ndarrays."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ser1 = pd.Series(['a', 'b', 'c'], index=[1, 2, 3])\nser2 = pd.Series(['d', 'e', 'f'], index=[4, 5, 6])\npd.concat([ser1, ser2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It also concatenates higher-dimensional objects, such as ``DataFrame``s:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df9 = pd.DataFrame({'A': ['a', 'c'],\n                    'B': ['b', 'd']})\ndf9",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.concat([df9, df9])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that `pd.concat` has preserved the indexing even though that means that it has been duplicated. You can have the results re-indexed (and avoid potential confusion down the road) like so:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.concat([df9, df9], ignore_index=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "By default, `pd.concat` concatenates row-wise within the `DataFrame` (that is, `axis=0` by default). You can specify the axis along which to concatenate:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.concat([df9, df9], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that while pandas will display this without error, you will get an error message if you try to assign this result as a new `DataFrame`. Column names in `DataFrame`s must be unique."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Concatenation with joins\nJust as you did with merge above, you can use inner and outer joins when concatenating `DataFrame`s with different sets of column names."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df10 = pd.DataFrame({'A': ['a', 'd'],\n                     'B': ['b', 'e'],\n                     'C': ['c', 'f']})\ndf10",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df11 = pd.DataFrame({'B': ['u', 'x'],\n                     'C': ['v', 'y'],\n                     'D': ['w', 'z']})\ndf11",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.concat([df10, df11])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As we saw earlier, the default join for this is an outer join and entries for which no data is available are filled with `NaN` values. You can also do an inner join:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.concat([df10, df11], join='inner')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another option is to directly specify the index of the remaininig colums using the `join_axes` argument, which takes a list of index objects. Here, we will specify that the returned columns should be the same as those of the first input (`df10`):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.concat([df10, df11], join_axes=[df10.columns])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### `append()`\n\nBecause direct array concatenation is so common, ``Series`` and ``DataFrame`` objects have an ``append`` method that can accomplish the same thing in fewer keystrokes. For example, rather than calling ``pd.concat([df9, df9])``, you can simply call ``df9.append(df9)``:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df9.append(df9)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Important point**: Unlike the `append()` and `extend()` methods of Python lists, the `append()` method in pandas does not modify the original object. It instead creates a new object with the combined data.\n\n> **Takeaway:** A large part of the value you can provide as a data scientist comes from connecting multiple, often disparate datasets to find new insights. Learning how to join and merge data is thus an essential part of your skill set."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Exploratory statistics and visualization\n\n> **Learning goal:** By the end of this subsection, you should be familiar with some of the ways to visually explore the data stored in `DataFrame`s.\n\nOften when probing a new data set, it is invaluable to get high-level information about what the dataset holds. Earlier in this section we discussed using methods such as `DataFrame.info`, `DataFrame.head`, and `DataFrame.tail` to examine some aspects of a `DataFrame`. While these methods are critical, they are on their own often insufficient to get enough information to know how to approach a new dataset. This is where exploratory statistics and visualizations for datasets come in.\n\nTo see what we mean in terms of gaining exploratory insight (both visually and numerically), let's dig into one of the the datasets that come with the scikit-learn library, the Boston Housing Dataset (though you will load it from a CSV file):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/housing_dataset.csv')\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This dataset contains information collected from the U.S Census Bureau concerning housing in the area of Boston, Massachusetts and was first published in 1978. The dataset has 14 columns:\n - **CRIM**:     Per-capita crime rate by town\n - **ZN**:       Proportion of residential land zoned for lots over 25,000 square feet\n - **INDUS**:    Proportion of non-retail business acres per town\n - **CHAS**:     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n - **NOX**:      Nitric oxides concentration (parts per 10 million)\n - **RM**:       Average number of rooms per dwelling\n - **AGE**:      Proportion of owner-occupied units built prior to 1940\n - **DIS**:      Weighted distances to five Boston employment centres\n - **RAD**:      Index of accessibility to radial highways\n - **TAX**:      Full-value property-tax rate per \\$10,000\n - **PTRATIO**:  Pupil-teacher ratio by town\n - **LSTAT**:    Percent of lower-status portion of the population\n - **MEDV**:     Median value of owner-occupied homes in \\$1,000s"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One of the first methods we can use to better understand this dataset is `DataFrame.shape`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The dataset has 506 rows and 13 columns.\n\nTo get a better idea of the contents of each column we can use `DataFrame.describe`, which returns the maximum value, minimums value, mean, and standard deviation of numeric values in each columns, in addition to the quartiles for each column:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Because dataset can have so many columns in them, it can often be useful to transpose the results of `DataFrame.describe` to better use them:"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that you can also examine specific descriptive statistics for columns without having to invoke `DataFrame.describe`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['MEDV'].mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['MEDV'].max()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['AGE'].median()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now find the maximum value in df['AGE'].\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Other information that you will often want to see is the relationship between different columns. You do this with the `DataFrame.groupby` method. For example, you could examine the average MEDV (median value of owner-occupied homes) for each value of AGE (proportion of owner-occupied units built prior to 1940):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.groupby(['AGE'])['MEDV'].mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Now try to find the median value for AGE for each value of MEDV.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also apply a lambda function to each element of a `DataFrame` column by using the `apply` method. For example, say you wanted to create a new column that flagged a row if more than 50 percent of owner-occupied homes were build before 1940:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['AGE_50'] = df['AGE'].apply(lambda x: x>50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Once applied, you also see how many values returned true and how many false by using the `value_counts` method:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['AGE_50'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also examine figures from the groupby statement you created earlier:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.groupby(['AGE_50'])['MEDV'].mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can also group by more than one variable, such AGE_50 (the one you just created), CHAS (whether a town is on the Charles River), and RAD (an index measuring access to the Boston-area radial highways), and then evaluate each group for the average median home price in that group:"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "groupby_twovar=df.groupby(['AGE_50','RAD','CHAS'])['MEDV'].mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can then see what values are in this stacked group of variables:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "groupby_twovar",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's take a moment to analyze these results in a little depth. The first row reports that communities with less the half of houses built before 1940, with a highway-access index of 1, and that are not situated on the Charles River have a mean house price of \\$24,667 (1970s dollars); the next row shows that for communities similar to the first row except for being located on the Charles River have a mean house price of \\$50,000.\n\nOne insight that pops out from continuing down this is that, all else being equal, being located next to the Charles River can significantly increase the value of newer housing stock. The story is more ambiguous for communities dominated by older houses: proximity to the Charles significantly increases home prices in one community (and that one presumably farther away from the city); for all others, being situated on the river either provided a modest increase in value or actually decreased mean home prices.\n\nWhile groupings like this can be a great way to begin to interrogate your data, you might not care for the 'tall' format it comes in. In that case, you can unstack the data into a \"wide\" format:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "groupby_twovar.unstack()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# How could you use groupby to get a sense of the proportion \n# of residential land zoned for lots over 25,000 sq.ft., \n# the proportion of non-retail business acres per town, \n# and the distance of towns from employment centers in Boston?\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "It is also often valuable to know how many unique values a column has in it with the `nunique` method:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['CHAS'].nunique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Complementary to that, you will also likely want to know what those unique values are, which is where the `unique` method helps:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['CHAS'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can use the `value_counts` method to see how many of each unique value there are in a column:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['CHAS'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Or you can easily plot a bar graph to visually see the breakdown:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "%matplotlib inline\ndf['CHAS'].value_counts().plot(kind='bar')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Note that the IPython magic command `%matplotlib inline` enables you to view the chart inline.\n\nLet's pull back to the dataset as a whole for a moment. Two major things that you will look for in almost any dataset are trends and relationships. A typical relationship between variables to explore is the Pearson correlation, or the extent to which two variables are linearly related. The `corr` method will show this in table format for all of the columns in a `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.corr(method='pearson')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Suppose you just wanted to look at the correlations between all of the columns and just one variable? Let's examine just the correlation between all other variables and the percentage of owner-occupied houses build before 1940 (AGE). We will do this by accessing the column by index number:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "corr = df.corr(method='pearson')\ncorr_with_homevalue = corr.iloc[-1]\ncorr_with_homevalue[corr_with_homevalue.argsort()[::-1]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "With the correlations arranged in descending order, it's easy to start to see some patterns. Correlating AGE with a variable we created from AGE is a trivial correlation. However, it is interesting to note that the percentage of older housing stock in communities strongly correlates with air pollution (NOX) and the proportion of non-retail business acres per town (INDUS); at least in 1978 metro Boston, older towns are more industrial.\n\nGraphically, we can see the correlations using a heatmap from the Seaborn library:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import seaborn as sns\nsns.heatmap(df.corr(),cmap=sns.cubehelix_palette(20, light=0.95, dark=0.15))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Histograms are another valuable tool for investigating your data. For example, what is the overall distribution of prices of owner-occupied houses in the Boston area?"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nplt.hist(df['MEDV'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The default bin size for the matplotlib histogram (essentially big of buckets of percentages that you include in each histogram bar in this case) is pretty large and might mask smaller details. To get a finer-grained view of the AGE column, you can manually increase the number of bins in the histogram:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "plt.hist(df['MEDV'],bins=50)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Seaborn has a somewhat more attractive version of the standard matplotlib histogram: the distribution plot. This is a combination histogram and kernel density estimate (KDE) plot (essentially a smoothed histogram):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.distplot(df['MEDV'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another commonly used plot is the Seaborn jointplot, which combines histograms for two columns along with a scatterplot:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.jointplot(df['RM'], df['MEDV'], kind='scatter')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Unfortunately, many of the dots print over each other. You can help address this by adding some alpha blending, a figure that sets the transparency for the dots so that concentrations of them drawing over one another will be apparent:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.jointplot(df['RM'], df['MEDV'], kind='scatter', alpha=0.3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Another way to see patterns in your data is with a two-dimensional KDE plot. Darker colors here represent a higher concentration of data points:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.kdeplot(df['RM'], df['MEDV'], shade=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Note that while the KDE plot is very good at showing concentrations of data points, finer structures like linear relationships (such as the clear relationship between the number of rooms in homes and the house price) are lost in the KDE plot.\n\nFinally, the pairplot in Seaborn allows you to see scatterplots and histograms for several columns in one table. Here we have played with some of the keywords to produce a more sophisticated and easier to read pairplot that incorporates both alpha blending and linear regression lines for the scatterplots."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.pairplot(df[['RM', 'AGE', 'LSTAT', 'DIS', 'MEDV']], kind=\"reg\", plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.1}})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Visualization is the start of the really cool, fun part of data science. So play around with these visualization tools and see what you can learn from the data!"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** An old joke goes: “What does a data scientist seen when they look at a dataset? A bunch of numbers.” There is more than a little truth in that joke. Visualization is often the key to finding patterns and correlations in your data. While visualization cannot often deliver precise results, it can point you in the right direction to ask better questions and efficiently find value in the data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Section 6: Introduction to machine learning models \n\nYou have now made it to the section on machine learning (ML). ML and the branch of computer science in which it resides, artificial intelligence (AI), are so central to data science that ML/AI and data science are synonymous in the minds of many people. However, the preceding sections have hopefully demonstrated that there are a lot of other facets to the discipline of data science apart from the prediction and classification tasks that supply so much value to the world. (Remember, at least 80 percent of the effort in most data-science projects will be composed of cleaning and manipulating the data to prepare it for analysis.)\n\nThat said, ML is fun! In this section, and the next one on data science in the cloud, you will get to play around with some of the “magic” of data science and start to put into practice the tools you have spent the last five sections learning. Let's get started!\n\n## A quick aside: types of ML\n\nAs you get deeper into data science, it might seem like there are a bewildering array of ML algorithms out there. However many you encounter, it can be handy to remember that most ML algorithms fall into three broad categories:\n - **Predictive algorithms**: These analyze current and historical facts to make predictions about unknown events, such as the future or customers’ choices.\n - **Classification algorithms**: These teach a program from a body of data, and the program then uses that learning to classify new observations.\n - **Time-series forecasting algorithms**: While it can argued that these algorithms are a part of predictive algorithms, their techniques are specialized enough that they in many ways functions like a separate category. Time-series forecasting is beyond the scope of this course, but we have more than enough work with focusing here on prediction and classification.\n\n## Prediction: linear regression\n\n> **Learning goal:** By the end of this subsection, you should be comfortable fitting linear regression models, and you should have some familiarity with interpreting their output.\n\nArguably the simplest form of machine learning is to draw a line connecting two points and make predictions about where that trend might lead.\n\nBut what if you have more than two points—and those points don't line up neatly? What if you have points in more than two dimensions? This is where linear regression comes in.\n\nFormally, linear regression is used to predict a quantitative *response* (the values on a Y axis) that is dependent on one or more *predictors* (values on one or more axes that are orthogonal to Y, commonly just thought of collectively as X). The working assumption is that the relationship between predictors and response is more or less linear. The goal of linear regression is to fit a straight line in the best possible way to minimize the deviation between our observed responses in the dataset and the responses predicted by our line, the linear approximation. (The most common means of assessing this error is called the **least squares method**; it consists of minimizing the number you get when you square the difference between your predicted value and the actual value and add up all of those squared differences for your entire dataset.)\n\n<img align=\"left\" style=\"padding-right:10px;\" src=\"Graphics/Sec6_linear_regression.png\">\n\nStatistically, we can represent this relationship between response and predictors as:\n\n$Y = B_0 + B_1X + E$\n\nRemember high school geometry? $B_0$ is the intercept of our line and $B_1$ is its slope. We commonly refer to $B_0$ and $B_1$ as coefficients and to $E$ as the *error term*, which represents the margin of error in the model.\n\nLet's try this in practice with actual data. (Note: no graph paper will be harmed in the course of these predictions.)\n\n### Data exploration\n\nWe'll begin by importing our usual libraries and using our %matplotlib inline magic command:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And now for our data. In this case, we’ll use a newer housing dataset than the Boston Housing Dataset we used in the last section (with this one storing data on individual houses across the United States)."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/Housing_Dataset_Sample.csv')\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Do you remember the DataFrame method for looking at overall information\n# about a DataFrame, such as number of columns and rows? Try it here.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's also use the `describe` method to look at some of the vital statistics about the columns. Note that in cases like this, in which some of the column names are long, it can be helpful to view the transposition of the summary, like so:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.describe().T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's look at the data in the **Price** column. (You can disregard the deprecation warning if it appears.)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.distplot(df['Price'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As we would hope with this much data, our prices form a nice bell-shaped, normally distributed curve.\n\nNow, let's look at a simple relationship like that between house prices and the average income in a geographic area:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.jointplot(df['Avg. Area Income'],df['Price'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As we would expect, there is an intuitive, linear relationship between them. Also good: the pairplot shows that the data in both columns is normally distributed, so we don't have to worry about somehow transforming the data for meaningful analysis.\n\nLet's take a quick look at all of the columns:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.pairplot(df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Some observations:\n1. Not all of the combinations of columns provide strong linear relationships; some just look like blobs. That's nothing to worry about for our analysis.\n2. See the visualizations that look like lanes rather than organic groups? That is the result of the average number of bedrooms in houses being measured in discrete values rather than continuous ones (as no one has 0.3 bedrooms in their house). The number of bathrooms is also the one column whose data is not really normally distributed, though some of this might be distortion caused by the default bin size of the pairplot histogram functionality.\n\nIt is now time to make a prediction. \n\n### Fitting the model\n\nLet's make a prediction. Let's feed everything into a linear model (average area income, average area house age, average area number of rooms, average area number of bedrooms, and\tarea population) and see how well knowing those factors can help us predict the price of a home. \n\nTo do this, we will make our first five columns the X (our predictors) and the **Price** column the Y (our response):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X = df.iloc[:,:5]\ny = df['Price']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we could use all of our data to create our model. However, all that would get us is a model that is good at predicting itself. Not only would that leave us with no objective way to measure how good the model is, it would also likely lead to a model that was less accurate when used on new data. Such a model is termed *overfitted*.\n\nTo avoid this, data scientists divide their datasets for ML into *training* data (the data used to fit the model) and *test* data (data used to evaluate how accurate the model is). Fortunately, scikit-learn provides a function that enables us to easily divide up our data between training and test sets: `train_test_split`. In this case, we will use 70 percent of our data for training and reserve 30 percent of it for testing. (Note that you will also supply a fourth parameter to the function: `random_state`; `train_test_split` randomly divides up our data between test and training, so this number provides an explicit seed for the random-number generator so that you will get the same result each time you run this code snippet.)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=54)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "All that is left now is to import our linear regression algorithm and fit our model based on our training data:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "reg.fit(X_train,y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Evaluating the model\n\nNow, a moment of truth: let's see how our model does making predictions based on the test data:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "predictions = reg.predict(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "predictions",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Our predictions are just an array of numbers: these are the house prices predicted by our model. One for every row in our test dataset.\n\nRemember how we mentioned that linear models have the mathematical form of $Y = B_0 + B_1*X + E$? Let’s look at the actual equation:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(reg.intercept_,reg.coef_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In algebraic terms, here is our model:\n\n$Y=-2,646,401+0.21587X_1+0.00002X_2+0.00001X_3+0.00279X_4+0.00002X_5$\n\nwhere:\n - $Y=$ Price\n - $X_1=$ Average area income\n - $X_2=$ Average area house age\n - $X_3=$ Average area number of rooms\n - $X_4=$ Average area number of bedrooms\n - $X_5=$ Area population\n\nSo, just how good is our model? There are many ways to measure the accuracy of ML models. Linear models have a good one: the $R^2$ score (also knows as the coefficient of determination). A high $R^2$, close to 1, indicates better prediction with less error."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Explained variation. A high R2 close to 1 indicates better prediction with less error.\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test,predictions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The $R^2$ score also indicates how much explanatory power a linear model has. In the case of our model, the five predictors we used in the model explain a little more than 92 percent of the price of a house in this dataset.\n\nWe can also plot our errors to get a visual sense of how wrong our predictions were:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#plot errors\nsns.distplot([y_test-predictions])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Do you notice the numbers on the left axis? Whereas a histogram shows the number of things that fall into discrete numeric buckets, a kernel density estimation (KDE, and the histogram that accompanies it in the Seaborn displot) normalizes those numbers to show what proportion of results lands in each bucket. Essentially, these are all decimal numbers less than 1.0 because the area under the KDE has to add up to 1.\n\nMaybe more gratifying, we can plot the predictions from our model:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Plot outputs\nplt.scatter(y_test,predictions, color='blue')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The linear nature of our predicted prices is clear enough, but there are so many of them that it is hard to tell where dots are concentrated. Can you think of a way to refine this visualization to make it clearer, particularly if you were explaining the results to someone?\n\n> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Hint: Remember to try the plt.scatter parameter alpha=.\n# It takes values between 0 and 1.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Takeaway:** In this subsection, you performed prediction using linear regression by exploring your data, then fitting your model, and finally evaluating your model’s performance."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Classification: logistic regression\n\n> **Learning goal:** By the end of this subsection, you should know how logistic regression differs from linear regression, be comfortable fitting logistic regression models, and have some familiarity with interpreting their output.\n\nWe'll now pivot to discussing classification. If our simple analogy of predictive analytics was drawing a line through points and extrapolating from that, then classification can be described in its simplest form as drawing lines around groups of points.\n\nWhile linear regression is used to predict quantitative responses, *logistic* regression is used for classification problems. Formally, logistic regression predicts the categorical response (Y) based on predictors (Xs). Logistic regression goes by several names, and it is also known in the scholarly literature as logit regression, maximum-entropy classification (MaxEnt), and the log-linear classifier. In this algorithm, the probabilities describing the possible outcomes of a single trial are modeled using a sigmoid (S-curve) function. Sigmoid functions take any value and transform it to be between 0 and 1, which can be used as a probability for a class to be predicted, with the goal of predictors mapping to 1 when something belongs in the class and 0 when they do not.\n\n<img align=\"left\" style=\"padding-right:10px;\" src=\"Graphics/Sec6_logistic_regression.png\">\n\nTo show this in action, let's do something a little different and try a historical dataset: the fates of the passengers of the RMS Titanic, which is a popular dataset for classification problems in machine learning. In this case, the class we want to predict is whether a passenger survived the doomed liner's sinking.\n\nThe dataset has 12 variables:\n\n - **PassengerId**\n - **Survived:** 0 = No, 1 = Yes\n - **Pclass:** Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n - **Sex**\n - **Age**\t\t\n - **Sibsp:** Number of siblings or spouses aboard the *Titanic*\t\n - **Parch:** Number of parents or children aboard the *Titanic*\n - **Ticket:** Passenger ticket number\t\n - **Fare:** Passenger fare\t\n - **Cabin:** Cabin number\t\n - **Embarked:** Port of embarkation; C = Cherbourg, Q = Queenstown, S = Southampton"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('Data/train_data_titanic.csv')\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One reason that the Titanic data set is a popular classification set is that it provides opportunities to prepare data for analysis. To prepare this dataset for analysis, we need to perform a number of tasks:\n - Remove extraneous variables\n - Check for multicollinearity \n - Handle missing values\n\nWe will touch on each of these steps in turn.\n\n### Remove extraneous variables\n\nThe name of individual passengers and their ticket numbers will clearly do nothing to help our model, so we can drop those columns to simplify matters."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.drop(['Name','Ticket'],axis=1,inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are additional variables that will not add classifying power to our model, but to find them we will need to look for correlation between variables.\n\n### Check for multicollinearity\n\nIf one or more of our predictors can themselves be predicted from other predictors, it can produce a state of *multicollinearity* in our model. Multicollinearity is a challenge because it can skew the results of regression models (both linear and logistic) and reduce the predictive or classifying power of a model.\n\nTo help combat this problem, we can start to look for some initial patterns. For example, do any correlations between **Survived** and **Fare** jump out?"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "sns.pairplot(df[['Survived','Fare']], dropna=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Try running sns.pairplot twice more on some other combinations of columns\n# and see if any patterns emerge.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also use `groupby` to look for patterns. Consider the mean values for the various variables when we group by **Survived**:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.groupby('Survived').mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Survivors appear to be slightly younger on average with higher-cost fare."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Value counts can also help us get a sense of the data before us, such as numbers for siblings and spouses on the *Titanic*, in addition to the sex split of passengers:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['SibSp'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['Parch'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['Sex'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Handle missing values\n\nWe now need to address missing values. First, let’s look to see which columns have more than half of their values missing:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#missing\ndf.isnull().sum()>(len(df)/2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's break down the code in the call above just a bit. `df.isnull().sum()` tells pandas to take the sum of all of the missing values for each column. `len(df)/2` is just another way of expressing half the number of rows in the `DataFrame`. Taken together with the `>`, this line of code is looking for any columns with more than half of its entries missing, and there is one: **Cabin**.\n\nWe could try to do something about those missing values. However, if any pattern does emerge in the data that involves **Cabin**, it will be highly cross-correlated with both **Pclass** and **Fare** (as higher-fare, better-class accommodations were grouped together on the *Titanic*). Given that too much cross-correlation can be detrimental to a model, it is probably just better for us to drop **Cabin** from our `DataFrame`:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.drop('Cabin',axis=1,inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's now run `info` to see if there are columns with just a few null values."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One note on the data: given that 1,503 died in the *Titanic* tragedy (and that we know that some survived), this data set clearly does not include every passenger on the ship (and none of the crew). Also remember that **Survived** is a variable that includes both those who survived and those who perished.\n\nBack to missing values. **Age** is missing several values, as is **Embarked**. Let's see how many values are missing from **Age**:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['Age'].isnull().value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As we saw above, **Age** isn't really correlated with **Fare**, so it is a variable that we want to eventually use in our model. That means that we need to do something with those missing values. But we before we decide on a strategy, we should check to see if our median age is the same for both sexes."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.groupby('Sex')['Age'].median().plot(kind='bar')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The median ages are different for men and women sailing on the *Titanic*, which means that we should handle the missing values accordingly. A sound strategy is to replace the missing ages for passengers with the median age *for the passengers' sexes*."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['Age'] = df.groupby('Sex')['Age'].apply(lambda x: x.fillna(x.median()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Any other missing values?"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.isnull().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We are missing two values for **Embarked**. Check to see how that variable breaks down:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['Embarked'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The vast majority of passengers embarked on the *Titanic* from Southampton, so we will just fill in those two missing values with the most statistically likely value (the median result): Southampton."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['Embarked'].fillna(df['Embarked'].value_counts().idxmax(), inplace=True)\ndf['Embarked'].value_counts()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = pd.get_dummies(data=df, columns=['Sex', 'Embarked'],drop_first=True)\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's do a final look at the correlation matrix to see if there is anything else we should remove."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.corr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Pclass** and **Fare** have some amount of correlation, we can probably get rid of one of them. In addition, we need to remove **Survived** from our X `DataFrame` because it will be our response `DataFrame`, Y:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X = df.drop(['Survived','Pclass'],axis=1)\ny = df['Survived']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=67)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**\n\nWe now need to split the training and test data, which you will so as an exercise:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n# Look up in the portion above on linear regression and use train_test_split here.\n# Set test_size = 0.3 and random_state = 67 to get the same results as below when\n# you run through the rest of the code example below.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you will import and fit the logistic regression model:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "lr.fit(X_train,y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "predictions = lr.predict(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Evaluate the model\n\nIn contrast to linear regression, logistic regression does not produce an $R^2$ score by which we can assess the accuracy of our model. In order to evaluate that, we will use a classification report, a confusion matrix, and the accuracy score.\n\n#### Classification report"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The classification reports the proportions of both survivors and non-survivors with four scores:\n - **Precision:** The number of true positives divided by the sum of true positives and false positives; closer to 1 is better.\n - **Recall:** The true-positive rate, the number of true positives divided by the sum of the true positives and the false negatives.\n - **F1 score:** The harmonic mean (the average for rates) of precision and recall.\n - **Support:** The number of true instances for each label.\n \nWhy so many ways of measuring accuracy for a model? Well, success means different things in different contexts. Imagine that we had a model to diagnose infectious disease. In such a case we might want to tune our model to maximize recall (and thus minimize our false-negative rate): even high precision might miss a lot of infected people. On the other hand, a weather-forecasting model might be interested in maximizing precision because the cost of false negatives is so low. For other uses, striking a balance between precision and recall by maximizing the F1 score might be the best choice. Run the classification report:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(classification_report(y_test,predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Confusion matrix\n\nThe confusion matrix is another way to present this same information, this time with raw scores. The columns show the true condition, positive on the left, negative on the right. The rows show predicted conditions, positive on the top, negative on the bottom. So, the matrix below shows that our model correctly predicted 146 survivors (true positives) and incorrectly predicted another 16 (false positives). On the other hand, our model correctly predicted 30 non-survivors (true negatives) and incorrectly predicted 76 more (false negatives)."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(confusion_matrix(y_test,predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's dress up the confusion matrix a bit to make it a little easier to read:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.DataFrame(confusion_matrix(y_test, predictions), columns=['True Survived', 'True Not Survived'], index=['Predicted Survived', 'Predicted Not Survived'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Accuracy score\n\nFinally, our accuracy score tells us the fraction of correctly classified samples; in this case (146 + 76) / (146 + 76 + 30 + 16)."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(accuracy_score(y_test,predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Not bad for an off-the-shelf model with no tuning!\n\n> **Takeaway:** In this subsection, you performed classification using logistic regression by removing extraneous variables, checking for multicollinearity, handling missing values, and fitting and evaluating your model."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Classification: decision trees\n\n> **Learning goal:** By the end of this subsection, you should be comfortable fitting decision-tree models and have some understanding of what they output.\n\nIf logistic regression uses observations about variables to swing a metaphorical needle between 0 and 1, classification based on decision trees programmatically builds a Yes/No decision to classify items.\n\n<img align=\"left\" style=\"padding-right:10px;\" src=\"Graphics/Sec6_decision_tree.png\">\n\nLet's look at this in practice with the same *Titanic* dataset we used with logistic regression."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn import tree",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tr = tree.DecisionTreeClassifier()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "> **Exercise:**"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Using the same split data as with the logistic regression,\n# can you fit the decision tree model?\n# Hint: Refer to code snippet for fitting the logistic regression above.\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tr.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Once fitted, we get our predicitions just like we did in the logistic regression example above:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "tr_predictions = tr.predict(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "pd.DataFrame(confusion_matrix(y_test, tr_predictions), \n             columns=['True Survived', 'True Not Survived'], \n             index=['Predicted Survived', 'Predicted Not Survived'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(accuracy_score(y_test,tr_predictions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One of the great attractions of decision trees is that the models are readable by humans. Let's visualize to see it in action. (Note, the generated graphic can be quite large, so scroll to the right if the generated graphic just looks blank at first.)"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import graphviz \n\ndot_file = tree.export_graphviz(tr, out_file=None, \n                                feature_names=X.columns, \n                                class_names='Survived',  \n                                filled=True,rounded=True)  \ngraph = graphviz.Source(dot_file)  \ngraph",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "There are, of course, myriad other ML models that we could explore. However, you now know some of the most commonly encountered ones, which is great preparation to understand what automated, cloud-based ML and AI services are doing and how to intelligently apply them to data-science problems, the subject of the next section."
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "> **Takeaway:** In this subsection, you performed classification on previously cleaned data by fitting and evaluating a decision tree."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "name": "_merged"
  },
  "nbformat": 4,
  "nbformat_minor": 1
}